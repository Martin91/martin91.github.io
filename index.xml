<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hackerpie</title><link>https://blog.hackerpie.com/</link><description>Recent content on Hackerpie</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Sat, 19 Feb 2022 15:38:20 +0800</lastBuildDate><atom:link href="https://blog.hackerpie.com/index.xml" rel="self" type="application/rss+xml"/><item><title>当反射 map[string]interface{} 遇上 MapIndex 方法，返回值的 Kind 不是具体类型？</title><link>https://blog.hackerpie.com/posts/reflect/why-reflect-map-index-function-returns-interface/</link><pubDate>Sat, 19 Feb 2022 15:38:20 +0800</pubDate><guid>https://blog.hackerpie.com/posts/reflect/why-reflect-map-index-function-returns-interface/</guid><description>什么是反射？ 反射是一种在运行时用于探测甚至修改内存数据以及程序行为的机制，在 go 语言中通过 reflect 包实现。直白来说，利用反射，我们可以实现包括但不限于的以下这些场景：
数据的反序列化，比如 json、yaml 等格式数据从纯文本到内存数据结构的反序列化过程 动态修改内存中的数据，比如创建新的字典数据、修改结构体的字段的值等 动态调用对象的某个方法或者包里的某个函数等 检查数据的类型以及对象的方法列表等 所以，这次想说什么问题呢？ 今天想分享的，是我前几天在一个使用 golang 反射功能对 map[string]interface{} 类型的数据做处理的过程中，遇到的一个反直觉的问题。下面是相关代码片断示例：
myData := map[string]interface{}{} json.Unmarshal(&amp;#34;{\&amp;#34;name\&amp;#34;: \&amp;#34;martin\&amp;#34;, \&amp;#34;score\&amp;#34;: 99}&amp;#34;, &amp;amp;myData) HandleData(myData) // 进行数据的处理过程 func HandleData(data interface{}) { value := reflect.ValueOf(data) // ... 其他代码 keyValue := value.MapIndex(reflect.ValueOf(&amp;#34;name&amp;#34;)) // 从数据中取对应键 name 的值，应该为 &amp;#34;martin&amp;#34; switch keyValue.Kind() { case reflect.String: doSth() // ... 其他 case，但是都没有包含 reflect.Interface 的匹配 } // ... 其他后续代码 } 在编写上面的代码的过程中，我期待程序会进入 case reflect.</description></item><item><title>Prefix or Suffix oh-my-zsh's shell prompt</title><link>https://blog.hackerpie.com/posts/skills/prefix-or-suffix-zsh-prompt/</link><pubDate>Wed, 16 Feb 2022 23:30:20 +0800</pubDate><guid>https://blog.hackerpie.com/posts/skills/prefix-or-suffix-zsh-prompt/</guid><description>Almost every developer loves his/her cool and colorful shell, so I install oh-my-zsh on my laptops and remote servers. However, as I use similar themes of oh-my-zsh and I also have the exactly same user name, most time I could not distinguish the actual machine on which I was operating.
I tried to search online about how to decorate the zsh prompt so that I can insert a machine label within it.</description></item><item><title>瞎聊：避免手握锤子看啥都是钉子</title><link>https://blog.hackerpie.com/posts/tittle-tattle/something-learned-from-techparty-blog-migration/</link><pubDate>Wed, 12 Jan 2022 11:24:20 +0800</pubDate><guid>https://blog.hackerpie.com/posts/tittle-tattle/something-learned-from-techparty-blog-migration/</guid><description>前两天在给 TechParty 设计和制作新的官网，选型了上线了作为网站设计和托管的平台。最后的一个大活就是如何将 TechParty 原来的 218 篇博客文章迁移到新的官网上，一站式管理。比较遗憾的是，上线了的博客系统并没有提供编程接口用于迁移已有博客，所以唯一的方式就是自行想办法将所有文章逐一拷贝到他们的富文本编辑器中。
TechParty 旧的博客系统是用的 Jekyll，一种主要使用 markdown 作为协作语言的静态博客站点生成工具，而上线了只支持富文本编辑器，也不支持 markdown，所以大体思路就是直接拷贝 Jekyll 生成好的 TechParty 博客的网页内容到上线了的编辑器中。但是问题是，这里面可是有 218 篇文章呢！不是一个小的工作量，所以作为程序员，第一反应就是尝试能不能将这个过程自动化？
尝试 python + selenium 第一个尝试的方案就是基于 Python + Selenium 的方式来实现整个过程的自动化。整个方案花了一下午大概4个小时的时间，包括解决 pip 依赖安装的问题、如何模拟鼠标长按选中博客文章正文的操作、如何模拟复制、如何模拟人工在上线了编辑器里点击和编辑等，最终整个方案因为无法完美模拟人工点击上线了的各个编辑框（为了进入编辑态，非编辑态时，页面上的各个控件显示为普通的文本）而失败……
人肉复制粘贴 第二天早上，痛定思痛，觉得自动化的路子虽然看起来比较聪明，但是实际上还有一些缺点或者未知问题：
可复用性低：因为这种从 Jekyll 博客到上线了博客迁移的需求显然对我自己是一次性的任务，而对于别人可能压根没有什么需求 未知的异常处理：哪怕我解决了交互模拟的问题，我还是无法预知脚本后续自动执行的过程中还会不会有其他问题需要解决，尽管脚本自动化很酷，但是干过的人都知道，它不是一劳永逸的事情，往往你还是需要人盯着，以备随时介入处理突发的情况，而且一旦修复完问题，你还需要脚本到底是从头再来，还是断点作业，问题只多不少，无穷无尽…… 在决定是否改用人工方式之前，我算了道简单的数学题：
迁移一篇文章的主要操作是：打开原文链接 -&amp;gt; 复制原文标题 -&amp;gt; 在上线了新建一篇博客 -&amp;gt; 粘贴标题 -&amp;gt; 回到原文复制原文发布时间 -&amp;gt; 粘贴到上线了的文章中 -&amp;gt; 复制原文正文 -&amp;gt; 粘贴到上线了的编辑器中 -&amp;gt; 点击上线了的“上线”按钮。如此往复就可以把所有文章都复制完。 以上一篇文章的复制操作，我大致觉得 60 秒绝对足够了，事实上对于熟练操作之后应该不需要这么长的时间 所以理论上复制所有文章需要的时间是： 218 x 60 = 13080 秒 = 218 分钟 = 3 小时 38 分钟 而更加合理一点的是按照每个文章 30 秒，那总体的时间就是减半的，也就是 1 小时 49 分钟 很有意思，当我想的是我需要完成 218 篇文章的搬迁的时候，我主观上觉得这是一个人肉操作难以短时间内完成的任务，至少可能要一整天都在干这个事情，除非用程序来自动完成。但是通过分解和评估，事实上不管是3个多小时还是2个小时，我都觉得这时间是要比写代码和调试程序的时间短的，而且结果可控的多：我很确定这是一个纯粹的时间问题，只要无脑操作，一定时间后，这个事情一定会完成，不用担心过程还有没有大的问题。</description></item><item><title>我的刻意练习——双拼输入</title><link>https://blog.hackerpie.com/posts/skills/double-input-method/</link><pubDate>Thu, 23 Dec 2021 14:38:20 +0800</pubDate><guid>https://blog.hackerpie.com/posts/skills/double-input-method/</guid><description>作为一个程序员，每天都需要和键盘打交道，自然少不了打字这个事情，以前就知道有五笔输入法，但是也听说练习起来很难，加上拼音输入法很简便，自然就使用的拼音输入法。但是久了之后，总是觉得全拼输入法太死板了，有没有不需要逐个打入每个拼音字母就可以快速打出需要的字的输入法的呢？答案是有的：双拼输入法，是的，就是我在写这篇文章时所使用的输入法。当然，我现在的打字速度就跟乌龟爬行一样慢……
什么是双拼输入法呢？ 与全拼输入法需要挨个输入拼音字母不同的是，双拼是将汉语拼音中的所有声母和韵母都映射到键盘上，所以打字的时候，我们的思维转回自然的拼音规律：声母+韵母，非常简单直观。换句话说，对于每个汉字，固定只需要 2 次敲击键盘就可以了，比起全拼自然是会更高效的，只是刚开始从全拼切换过来确实很痛苦，我自己也是在刻意练习中。比如“双拼输入法”这五个字，对应的拼音就是：
shuang pin shu ru fa 按照声母和韵母分开的话，就是：
sh uang p in sh u r u f a 以小鹤双拼来讲，对应的键盘输入顺序就是：u、l、p、b、u、u、r、u、f、a。
所以双拼输入法的原理极其简单，但是练习起来还是有一些痛苦的。
个人练习的心得体会 第一周 第一周的打字速度跟龟速一样，基本上是打每个字都得停顿一下提前想下接下来每个字的拼音以及对应的码键。这期间，我可以把我的电脑和手机的输入法都改为双拼，使用的小鹤双拼，虽然打字速度慢得叫人别扭，但是还是可以勉强忍受的。
第二周 经过第一周的刻意练习后，基本上就能记住每个双拼码的键位了，所以打字速度有一定程度的提升，这个阶段的最大问题就是经常不自觉就按照全拼的输入法来打字了，比如想要打“成”字，一打得快了，就是先打了“c”，实际上应该是“i”。又或者是“就”这个字，应该是“jq”，但是总是一顺手就打成了“ji”，哪怕现在已经是第三周了，也是如此。除了输入习惯的错误，另一个错误就是没有选对字，因为打字的时候注意力都在思考键位和纠正输入的键上，所以总是不自觉刚把拼音打完，就直接空格了，结果很可能选到的词是错误的。
第三周（此刻是2022-01-12的上午） 第三周会和第二周差不多，会稍微更注意候选词的问题，但是仍旧没有彻底改掉全拼遗留的输入习惯，容易一快了就敲错键，所以大部分时候的打字都还是需要在退格和重新输入中往复。不过好的一点是，打字速度还是有一些提升的，一些常用字可以很快甚至肌肉记忆般敲出来了，整体顺利和流畅的话，也能感受到双拼带给人的节奏感，期待未来某一天能够靠着双拼输入法一“写”千里。
感悟 刻意练习，就是决心把自己丢进不适应的状态中，并且尝试在不改变环境的前提下努力改变状态，比如我一开始就故意将自己的所有设备的输入法都改为双拼，后来跟别人聊天的时候，发现自己打字好慢，还特别多错字，特别着急，但是这个时候我也没有去临时改回全拼，而是坚持继续用双拼，慢慢就发现打字速度还是能提升，也就再也没有必要考虑改回双拼输入了。想起以前我纠正自己打字时候的指法也是一样的道理。 好看的码表图 以下图片是我从知乎讨论区（文末附了链接）看到的一个图片，我觉得比较好看，就拿来当壁纸了，以备对双拼码表不熟悉的时候可以直接回到桌面看看，但是实际上也基本没有用上，分享给有需要的同学。 学习资料 知乎：怎样记忆双拼输入法的回答</description></item><item><title>Unicode？UTF-8？GBK？……聊聊字符集和字符编码格式</title><link>https://blog.hackerpie.com/posts/text-processing/character-sets-and-encoding-formats/</link><pubDate>Sun, 12 Dec 2021 18:25:20 +0800</pubDate><guid>https://blog.hackerpie.com/posts/text-processing/character-sets-and-encoding-formats/</guid><description>按照习惯起个调 作为程序员，经常会在编程语言、操作系统、网络以及文本编辑等多个层面遇上字符集或者字符编码的问题，尽管一般都能快速通过搜索引擎找到解决方案，但是对于这种字符集以及其相关的字符编码格式的知识，倒是未曾系统梳理。恰逢近期有了一些收获，趁热记录分享下。
从 Unicode 和 UTF-8 说起 对于类 Unix 操作系统（比如 Mac OS 以及 Linux 操作系统等）的用户来说，会更多地接触 UTF-8 编码格式，我也是其中一个。而我过往总是容易跟另一个词—— Unicode 混淆，所以，当我们在讨论 UTF-8 和 Unicode 的时候，我们在讨论什么？
Unicode 字符集简介 当我们说 Unicode 的时候，是在讨论一种字符集（character set）。Unicode 翻译成中文叫“统一码”，是一种可以简单理解为收录了世界上所有语言的文字和符号的全球标准。大家知道，英语的基本组成元素是 26 个英文字母加上各种标点符号；而汉语的文字则相对繁杂，大量汉字，每个文字都有各自的拼音，拼音里还要区分音调，这里提到的汉字、拼音、音调以及汉字自身的标点符号，跟英语的英文字母以及标点符号等，统统收录在了 Unicode 字符集中，而类似的，还有繁体中文、日文、韩文、俄罗斯语、越南语、泰语、蒙古语等等。
收录了这么多的字符，就会带来一个问题：怎么整理和编排记录这些内容呢？编号！类比在一些常见的场景中，当一个集体中包含很多的个体时，为了用一种统一且简单的方式区分，我们最容易想到的就是编号。比如，给班里的同学安排座位号，给学生安排学号，给员工安排工号，等等。
但是，计算机是不能直接理解十进制这种人类易于理解的数字的，它只能理解二进制的数值，所以，在计算机里，我们可以用编码（使用特定的二进制序列来表示一个特定的值）的方式来给这些字符和符号进行一一映射。目前 Unicode 实际应用版本 UCS-2 在计算机中使用了 2 个字节来编码一个字符，也就是 16 位的编码空间，在表示上，采用类如 U+???? 的形式，其中每个“?”都是一个十六进制数。注意，Unicode 还有个 4 字节编码版本，亦即 UCS-4，不在这里讨论。
以下是一些示例的 Unicode 字符及其对应编码：
字符 编码值 说明 牛 U+725B 汉字 ù U+00F9 拼音 u 的四声 , U+002C 英文逗号 ， U+FF0C 中文逗号 😁 U+D83D emoji 表情：笑脸 ⚔ U+2694 emoji 表情：剑 是不是挺有意思的？另外是否也注意到，同样是逗号，但是英文的逗号和中文的逗号，并不是同一个符号，哪怕看起来非常相似！相信很多初学编程的同学也都踩过在代码中输入了中文逗号导致代码编译出错的坑吧！</description></item><item><title>Kafka 核心设计思考——来自官方文档的总结</title><link>https://blog.hackerpie.com/posts/kafka/kakfa-main-design/</link><pubDate>Thu, 09 Dec 2021 10:16:20 +0800</pubDate><guid>https://blog.hackerpie.com/posts/kafka/kakfa-main-design/</guid><description>前言 最近在学习 Kafka 的一些设计原理，偶然间发现 Kafka 官方文档中独列了 Design 一章。两天看完后觉得很兴奋，因为文档中很详细地从各方面阐述了 Kafka 官方对于 Kafka 设计的目标以及设计权衡等，让我恍然大悟 Kafka 的独特与简洁。这种快乐是阅读网上各种零散的博客文章无法比拟的。我此处总结更多是为了提升自己的领悟和理解程度，行文之中会夹杂个人主观理解，我建议大家抽出时间阅读原汁原味的官方文档。
Kafka 设计目标与设计概述 设计一个系统，精准的目标是第一步。Kafka 官方在最开始的时候，对 Kafka 的设计理想是将其做成一个可以帮助大型公司应对各种可能的实时数据流处理的通用平台。这句话里边有几个重点：“大型公司”、“实时”、“通用”，对应到系统设计上，就是需要支持大量数据的低延迟处理，并且需要考虑各种不同的数据处理场景。在官方阐述中，Kafka 着眼于以下几个核心指标：
高吞吐量：因为 Kafka 需要处理大量的消息； 低延迟：消息系统的关键设计指标； 支持加载离线数据：这是 Kafka 考虑的所谓“各种可能的”数据处理场景，支持从离线系统中加载数据，或者将数据加载到离线系统中，都是无法逃避的； 支持分区的、分布式的、实时的数据流处理以产生新的、派生的数据流：这个指导了 Kafka 里 topic 分区模型以及消费者模型的设计； 容错与可靠性：Kafka 作为消息中间件，核心场景之一就是作为系统间的连接器，需要保证整体业务的正常运作，可靠的消息投递机制以及应对节点故障的高可用设计等，必不可少。 理解了 Kafka 的设计目标以及核心指标，后续对 Kafka 的整体架构设计就会有一个方向了，因为 Kafka 的整体设计细节还算比较多，但是归根结底都是围绕这几个核心指标去做的设计，我尝试分门别类先汇总一下，可能不是很准确，希望请大家看的时候顺便赐教：
核心指标 实现的角度 具体设计手段 高吞吐量 读写缓存 依赖文件系统自身的 Page Cache，而不是自己实现内存缓存 高吞吐量 高效的数据结构 采用顺序读写的结构，而不是 B 树等 高吞吐量 降低大量小的 I/O 消息分批发布，按批投递 高吞吐量 提高消息投递吞吐量 由消费者批量拉取 高吞吐量 支持分批消息 支持异步发送消息 低延迟 避免昂贵的字节拷贝 统一的消息格式，零拷贝技术 低延迟 优化传输性能 通过批量消息压缩减小传输数据量 低延迟 提升读取性能 顺序读，日志文件分段存储，应用二分查找 低延迟 降低负载均衡延迟 producer 直连 broker 离线数据加载 支持周期性大量数据加载 依赖存储层顺序读写的常量时间复杂度的访问优势以及低廉的磁盘成本要求 离线数据处理 支持并行处理 通过分区设计以及 consumer 的 offset，支持 Hadoop 一类的并行作业以及断点作业 可靠性 支持“有且仅有一次”的消息投递语义 producer 的 ID 与消息 Sequence Number，类事务提交语义 可靠性 容错处理与高可用 ISR 机制与 Leader 均匀分布设计 除了上表所列内容，还有少量设计思考暂时不好归类，比如：</description></item><item><title>后缀表达式之逆波兰表示法</title><link>https://blog.hackerpie.com/posts/algorithms/queue-and-stack/reverse-polish-representation/</link><pubDate>Sun, 05 Dec 2021 20:56:28 +0800</pubDate><guid>https://blog.hackerpie.com/posts/algorithms/queue-and-stack/reverse-polish-representation/</guid><description>从中缀表达式说起 对于人类来说，中缀表达式是最直观自然的，比如“3+5x4”或者“(3+5)x4”，一般来说，对于中缀表达式，在程序中会用一个抽象语法树来表示表达式和求值，比如：
3+5x4 + / \ / \ 3 x / \ / \ 5 4 -------------------------------- (3+5)x4 x / \ / \ + 4 / \ / \ 3 5 后续表达式求值使用二叉树的中序遍历便可。
但是这种表达式对于计算机来说，会有2个可以考虑提升的问题：
对于计算机不够直观，需要在树的结构上进行遍历和求值； 额外的括号来用于明确运算优先级。 后缀表达式 后缀表达式，也叫逆波兰表达式，前述的表达式对应的后缀表达式为：
3+5x4：3 5 4 x + (3+5)x4：3 5 + 4 x 可以看出后缀表达式的特点：
操作符在操作数的末尾，比如 5 x 4 表示为 5 4 x； 无需括号表达优先级 从计算机的角度，后缀表达式还有以下特点：
由于没有括号，可以节省内存 可以基于栈结构实现后缀表达式的求值 如果对抽象语法树进行末序遍历，刚好可以得到逆波兰表达式，这点比较有意思 将中缀表达式转为后缀表达式 为了将中缀表达式转为后缀表达式，一般需要用到的是调度场算法，算法中需要用到一个输出队列和一个操作符栈，完整的算法细节比较多，这里简化为简单的四则运算（支持括号）来描述精简版算法，如果需要支持完整的运算符或者函数等，需要自行学习完整的调度场算法。</description></item><item><title>技术面试如何“试”</title><link>https://blog.hackerpie.com/posts/2021/interview-thinking/</link><pubDate>Sun, 07 Nov 2021 13:03:10 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2021/interview-thinking/</guid><description>毕业7年了，经历过多家不同公司的面试，这些公司的面试方式不尽相同，给我的喜恶程度不一；反过来，自己作为面试官，前后也为公司筛选物色了不少候选人，我希望我所认可的人能在后续的工作表现中证明我没看错人。我时不时会想：在技术面试中，以怎样的方式对候选人进行评估筛选，才足够高效精准？不妨先从自己所经历过的面试形式聊起吧。
我所经历过的几类面试风格 细想一路走来，我所经历过的面试风格大致分为几类：
全凭一张嘴，双方直接聊 家庭作业型 理论知识问答 算法能力考查 全凭一张嘴，双方直接聊 这种面试风格，存在于早期的互联网企业面试中，而据我了解，目前一些小公司也会采用这种方式。就我自己来说，15年我面试 4399 以及大疆的时候，均是采用的这种风格的面试。
面试双方就简历上的项目展开讨论，了解候选人在项目中的具体工作和成果，以及候选人在编程素养等方面的水平。这种面试方式简单快速，但是有效与否，很大程度上依赖候选人的表达能力以及面试官自身经验能力水平以及对人才的判断能力。
采用这种面试风格的公司，大抵是一些小型公司或者初创公司，因为这类公司一般相对难以吸引到拥有“优秀”背景的人才前来面试，除非事前双方都是知根知底的，于是公司必然会在人才招募上务实。如果本身吸引到的人才数量有限背景有限，公司自己还平添很多繁琐的面试环节或者提高通过门槛，最终可能一个人都招不到。所以，开门见山，直奔主题。
家庭作业型 这种面试形式，是在简历通过筛选后，面试官给候选人发送一份家庭作业一样的项目需求，描述一个极小型项目的设计要求，然后让候选人自己在业余时间完成。之后的技术面试则一方面结合这个小项目进行实现思路以及架构设计交流，另一方面则通过过往项目经验了解候选人的能力水平。我在 15 年面试 ThoughtWorks 以及 20 年面试 crypto.com 公司的时候，都体验过这种面试形式，也是我个人最为喜欢的形式。而根据朋友的分享，我了解到 AWS 中国也是采用的此类面试形式。
我之所以喜欢这种面试风格，主要是因为方式灵活以及代码更容易让技术面试双方建立共同语言。灵活性方面，它给我自己发挥的空间最大，比如我想向面试官展现我的开发习惯，比如单元测试和代码注释等，那我就会在这种作业型项目上用心完善单测和注释。其次我也会注意文档的编写和注释，特别是一份帮助面试官方便快速运行你的代码的 README 文档。这些，都不需要像其他面试一样临场发挥，我只需要思考如何准备得更好就行了。另一方面，代码是技术人之间的共同语言，这种方式拉近了我和面试官之间的距离。比如我在去年面试 crypto.com 的过程中，我在交完作业到开始正式面试中间的这段时间里，我和 crypto.com 的面试官在微信上就已经开始就代码问题进行了多次讨论。在这种非正式的交流过程中，我得以一种比较轻松自在的方式去表达我的思考，而面试官也在此过程中向我展示了他自身的技术水平，这种方式让我感觉就是在进行如常的技术交流而已，仿佛是在一个同事或者同行一起探讨而已。
尽管我自己喜欢这种面试风格，但是身边还是会有朋友或者同事并不认同这种方式，甚至觉得恶心。主要原因有两种，一是这种方式会更多地占用候选人的业余时间，二是这种方式可能会存在候选人作弊，由他人代为完成了作业。只不过我真觉得这两对我而言都不是问题。假如我需要追求一个未来3-5年适合我的公司，我觉得充分的准备以完整展现自己，是值得投入时间的。而第二个问题，我觉得配合面试流程中对作业中的实现细节以及架构思路等进行讨论，是可以快速判断这个代码是不是候选人自己独立完成的。
理论知识问答 这种就是大家常说的“八股文”了，就是面试官按照题库给出数据结构、数据库、计算机网络以及编程语言等等科目相关的基础知识问题，由候选人做出正确的回答。大家常吐槽的题目比如红黑树的原理、MySQL innoDB 存储引擎的索引设计、TCP 三次握手/四次挥手以及 Golang 语言的 goroutine 调度原理等等。而由于这种面试环节中的题目过于基础和客观，导致候选人不得不在面试之前花费大量的时间和精力进行复习，但是入职后在工作岗位上却由于没有实践机会后快速遗忘，留下了“面试造火箭，日常拧螺丝”的调侃。
至于采用这种面试风格的公司，皆是各类校招社招人才热捧的大公司，比如国内的 BAT，美团字节以及 Shopee 等等。这些公司所提供的薪资水平在业界出于中上甚至天花板水平，自然每年都能够吸引到大量优秀人才前往应聘。
我自己在去年面试腾讯和字节的经历中，一路摸爬滚打，每天起早摸黑复习基础知识，或者是根据面试过程中暴露的薄弱知识点进行强化复习，最终才终于通过这些公司的技术基础关。
尽管我并不喜欢这种方式，但是在 Shopee 公司担任面试官的时候，按照部门统一面试要求，我还是不得不机械地从题库中挑选各个知识点的题目，逐一向候选人提问。在一个半小时的时间里，我需要按序完成对候选人项目经验、编程语言基础、数据结构、计算机网络、数据库理论、操作系统原理、网络安全的理论知识考查，另外还包含一道中等难度的编程题，因为面试评估采用各环节得分累加的形式，我无法跳过其中某一部分。我仅有的发挥空间，大概就是面试开场的项目经验交流以及对候选人的回答决定是否追问了。
这种面试形式，更像是一种应试考核，尽管不够个性化，但是却仍然不失为一种筛选人才的方式。因为本身这类热门公司就能够吸引到超级多优秀人才，在简历筛选环节留下来的人选，绝大部分人基础都不会太差，基本素养也不至于太糟糕，而大公司在实现一种盈利机制的稳定之后，并不需要太多领军型人才，更多是需要一些踏实勤奋的人去保证这套机制的运作如常，甚至允许部分蛀虫的存在。在这种背景下，大公司的面试筛选机制便可以相对简单粗暴，公司只需要确保挑选出来的人在某个角度客观上比其他人突出即可。古代科举考试以及现代高考或者公务员考试制度，大致如此，虽说无法挖掘人的特长或者个性，但是确实公平。
算法能力考查 除开上面几种类型，我遇到的这种类型特点是面试绝不寒暄，也不多聊理论，简单自我介绍后，面试官直接给出算法题目，要求直接完成算法题实现。去年在 flexport 公司就是这种面试形式，当然，我没有通过。而众所周知，谷歌、微软、字节跳动、pony.ai 等公司都是比较重视算法能力的，如果候选人想要追求这类公司的岗位机会，算法能力太差是注定不行的。而今年在和一个海归的朋友聊的时候，他也聊到美企普遍重视算法能力，所以面试多是简单粗暴的多道算法题。这就让人忍不住又得提一下 Homebrew 作者 Max Howell 因为没有完成翻转二叉树的算法题而没有通过谷歌面试的故事。
尽管好的算法对于软件设计来说确实可以降本增效，但是对于大部分工程师来说，更多时候他们需要完成的是如何通过技术方案的组合来给出一个商业产品的解决方案。这也让我想起18年底的时候，一个刚入职微软半年左右的朋友，通过猎头找到我，希望跟我了解 Shopee 的工作情况，他正在考虑跳槽 Shopee。因为他觉得入职微软之后大失所望，尽管他准备微软的面试上花了很大力气，微软在他的想象里，一直是个高效专业的公司，但是实际他当时所在的团队所维护的代码，实在难以恭维，而且很多所学毫无用武之地。
小结 这里我们先汇总对比下几种面试风格：
形式 形式简述 举例公司 好处 不足 纯口头交流 全程口头沟通，交流技术想法等 早期的大疆、4399 简单快速，成本低 重度依赖候选人表达能力以及面试官慧眼识人能力 家庭作业型 面试前完成作业，面试中交流作业实现思路等 thoughtworks、crypto.</description></item><item><title>部分应用与柯理化</title><link>https://blog.hackerpie.com/posts/programming-paradigm/partial-application-and-currying/</link><pubDate>Sun, 15 Aug 2021 21:18:18 +0800</pubDate><guid>https://blog.hackerpie.com/posts/programming-paradigm/partial-application-and-currying/</guid><description>在最近学习函数式编程的过程中，反复接触到的就是“柯理化”这个概念，特别数学范有没有？虽然看过多次，但是一直不是很好地理解它，恰逢今天在阅读《Scala 函数式编程》这本书的过程中加深了理解，便写个文章，总结一下。
柯理化 柯理化，英文叫“Currying”，命名源自逻辑学家 Haskell Curry 的名字。在数学和编程领域，柯理化用于将一个接收多个参数的函数转换为一系列只接收单个输入参数的函数。比如，将一个接收三个参数的函数 f 进行柯理化，会得到三个新的函数：
x = f(a, b, c) 变为： h = g(a) i = h(b) x = i(c) 或者使用匿名函数按序调用的形式，则为： x = g(a)(b)(c) 这样讲或许仍有点不好理解，我们用个数学函数的例子来分解。假如我们有函数 f(a, b, c) = a² + b - c，并且有 a = 2、b = 3、c = 1，则一般数学求解过程中，我们可以直接将 a、b、c 的值对应代入函数右侧式子，得到 2² + 3 - 1 = 6，于是我们知道 f(2, 3, 1) = 6。这个过程很直观很好理解，也很亲切对不对？
但是，假如我们要求每次只能代入函数的一个输入值，会是怎样的过程呢？
第一步，我们代入 a = 2，我们将得到 f(2, b, c) = 2² + b - c，我们可以记 g(b, c) = f(2, b, c) = 4 + b - c； 第二步，我们代入 b = 3，我们得到 g(3, c) = 4 + 3 - c，我们可以记 h(c) = g(3, c) = 7 - c； 最后一步，我们代入 c = 1，我们得到 h(1) = 7 - 1 = 6。 上述的过程，向我们展示了我们是如何通过每次代入一个输入值而得到一个输入值数量减 1 的新函数。</description></item><item><title>数据结构：单调栈</title><link>https://blog.hackerpie.com/posts/algorithms/monotonous-stacks/monotonous-stacks/</link><pubDate>Wed, 04 Aug 2021 23:13:28 +0800</pubDate><guid>https://blog.hackerpie.com/posts/algorithms/monotonous-stacks/monotonous-stacks/</guid><description>什么是单调栈 单调栈是指从栈顶到栈底，栈内元素的值符合单调性的一种特殊数据结构。从栈顶到栈底，元素的值单调递减，称为单调递减栈；反之，称为单调递增栈。
\ 9 / \ 1 / | 7 | | 3 | | 5 | | 5 | | 3 | | 7 | | 2 | | 9 | ------- ------- 单调递减栈 单调递增栈 单调栈的维护 为了维持栈的单调性，在往栈内插入元素时，需要比较循环比较栈顶元素与待插入元素的值的大小，以单调递增栈举例，需要始终确保栈顶元素的值大于等于待插入元素的值方可插入，否则需要先弹出栈顶元素之后，重复“检查-弹出”的流程，直到栈为空，或者栈顶元素的值大于等于待插入元素的值。
假设需要插入的元素按照序列 5, 2, 3, 7, 1 从左到右遍历，且需要维护单调递增栈，则插入过程为：
\ / \ / \ / \ / \ / | | | 2 | | 3 | | | | 1 | | 5 | | 5 | | 5 | | 7 | | 7 | ------- ------- ------- ------- ------- (1) (2) (3) (4) (5) (1) 待插入 5，栈为空，直接插入 5；</description></item><item><title>算法题解：二叉树寻路</title><link>https://blog.hackerpie.com/posts/algorithms/binary-tree/binary-tree-routine/</link><pubDate>Sat, 31 Jul 2021 18:20:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/algorithms/binary-tree/binary-tree-routine/</guid><description>本题来自 Leetcode 的 1104 题，是一道很有趣的考察二叉树数据结构的题，同时由于二叉树父子节点之间的特殊关系，同时还可以运用到位运算来巧妙解题。
先贴一下题目：
在一棵无限的二叉树上，每个节点都有两个子节点，树中的节点 逐行 依次按 “之” 字形进行标记。
如下图所示，在奇数行（即，第一行、第三行、第五行……）中，按从左到右的顺序进行标记；
而偶数行（即，第二行、第四行、第六行……）中，按从右到左的顺序进行标记。 给你树上某一个节点的标号 label，请你返回从根节点到该标号为 label 节点的路径，该路径是由途经的节点标号所组成的。
示例 1：
输入：label = 14 输出：[1,3,4,14] 示例 2：
输入：label = 26 输出：[1,2,6,10,26] 算法题解思路1：运用二叉树的节点的数值特性推导出公式求解 观察这个“之”字形二叉树，我们可以得出几个特点：
假如所有节点都是按照从左到右依次递增，按照二叉树的特性，我们可以归纳总结出： 记 vi = 某个节点的数值 v(左子节点) = 2 x vi v(右子节点) = 2 x vi + 1 相反： v(父节点) = vi / 2 对于每层(第一层为根节点)的第一个和最后一个节点，会有： v（第一个节点）= 2^(n-1) // 2 的 n-1 次方，n为当前层数 v（最后一个节点）= 2^n - 1 // 2 的 n 次方减 1，n为当前层数 对于任意一个数值，可以求出其所在的层数为： level = log2(N) + 1 从根节点开始，所有奇数层的节点是从左到右依次递增的；而所有偶数层的节点是从右到左依次递增的； 对于某一层的所有节点来说，它们都是一个等差数列，所以数列对称位置上的两个节点数值之和总是相等，即第一个节点和最后一个节点的值之和一定等于第二个节点和倒数第二个节点的值之和。结合第 2 点，这个和始终为 2^(n-1) + 2^n - 1。 结合以上5点性质，我们写出求任意一个节点的伪代码为：</description></item><item><title>算法题解：扣分后的最大得分</title><link>https://blog.hackerpie.com/posts/algorithms/dynamic-programming/maximum-number-of-points-with-cost/</link><pubDate>Sun, 18 Jul 2021 16:48:43 +0800</pubDate><guid>https://blog.hackerpie.com/posts/algorithms/dynamic-programming/maximum-number-of-points-with-cost/</guid><description>题目来自 Leetcode 的 5815 题。
题目的核心是：从二维矩阵中的每行中选取一个格子，每次选择一个格子后，所累计的最新积分等于前面已获积分加上被选格子的分数减去上一个格子和当前被选格子的列差。用公式表达更清晰：
points // 表示 m x n 的二维矩阵每个格子的分数 score(r, c) // 表示选取到 r 行 c 列所获得的最大得分 score(r+1, c^) = score(r, c) + points[r+1, c^] - abs(c - c^) 通过这个关系，可以确定两个事情：
这是一个典型的动态规划问题：问题的最优解依赖子问题的最优解，且子问题的最优解相互影响，这一点是和贪心算法最大的不同； 在为每一行选择一个格子时，要使 score(r+1, c^) 的值最大，需要找到一对特殊的 (c, c^) 的值，这也就是意味着：每次在为每一行挑选最优的格子时，需要针对结合上一行的每一列，与当前行的每一列，找出最优组合。 按照这个思路来写代码的话，整个算法的时间复杂度是 O(RC²)，空间复杂度是 O(C)。直接提交，会触发 TLE（Time Limit Exceed）。
优化思路 回到最开始列的式子那里，调整式子的写法：
score(r+1, c^) = score(r, c) - abs(c-c^) + points[r+1, c^] 可见，其中 points[r+1, c^] 是不变量，要使 score(r+1, c^) 的值最大，只需要满足 score(r, c) - abs(c-c^) 最大即可。</description></item><item><title>依赖倒置原则</title><link>https://blog.hackerpie.com/posts/2021/dependency-inversion-principle-introduce/</link><pubDate>Wed, 07 Jul 2021 21:22:20 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2021/dependency-inversion-principle-introduce/</guid><description>说起依赖倒置原则，已经不是个新鲜的词了，虽然也知道依赖倒置原则的具体设计模式，但是一直觉得难以理解何为“倒置”，直到今晚重新静心阅读了 Wikipedia 才恍然大悟！欣喜之余，赶紧写篇文章总结。
传统软件分层设计模式 在软件设计开发的时候，我们都会自然而然思考系统的分层设计，比如以一个典型的三层架构来举例：
--------------- 服务层 （提供 API 服务） --------------- ↓ --------------- 业务逻辑层 （封装具体的业务逻辑） --------------- ↓ --------------- 存储层 （处理数据存取） --------------- 依照此架构设计，我们可能自然而然地将各层代码实现直接封装在三个不同的代码包，其中 package.service 直接依赖 package.business，而 package.business 则直接依赖 package.repository。因此形成以下链式依赖链：
package.service ---&amp;gt; package.business ---&amp;gt; package.repository 这种分层代码设计风格直接耦合了依赖双方的实现，假如被依赖的包需要修改代码逻辑，则很可能导致依赖它的上层代码需要相应修改，极端场景下，这种耦合带来的变动影响可能扩散到整个依赖链。
其次，由于上层代码依赖了下层代码的具体实现，导致了上层代码的可复用性降低。举个具体例子，我们有一个运行了很久的系统，出于技术考量，我们需要将其存储层从 MySQL 移植到 MongoDB 上，而整个系统的核心业务逻辑并不需要也不应该有任何改变，如果是采用上述这种分层架构，则会导致我们除了替换存储层代码实现，还要相应修改业务逻辑层的代码，这就是我说的直接依赖实现会降低依赖一方的可复用性降低。
依赖倒置原则 先照本宣科讲下依赖倒置原则的含义：
高层级的模块不应该依赖低层级的模块。它们都应该依赖抽象（比如，接口）
抽象不应该依赖实现细节。实现细节（具体的实现）应该依赖抽象
有点抽象，有点拗口，有点无情，有点无理取闹对不对？
还是尝试用大白话解释一下：
高层级的模块应该依赖的是低层级的模块的行为的抽象，取决于具体编程语言，可以是抽象类或者接口等技术； 第2句话其实很简单，只有一个意思：只要依赖了实现，就是耦合了代码，所以我们需要始终依赖的是抽象，而不是实现。 将上面举的例子按照依赖倒置原则设计，就是这样子了：
package.service ---&amp;gt; package.business.interface ↑ ↑ 实现 ↑ package.business ---&amp;gt; package.repository.interface ↑ ↑ 实现 ↑ package.repository 以上面的模式来说，package.service 不再直接依赖于 package.business，而是依赖了 package.</description></item><item><title>对于测试数据管理的思考</title><link>https://blog.hackerpie.com/posts/2021/test-data-management-thinking/</link><pubDate>Sat, 03 Jul 2021 12:25:10 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2021/test-data-management-thinking/</guid><description>背景 在研发流程管理中，测试环节，不管是白盒测试还是黑盒测试，都是确保研发交付质量的关键。在过往的工作经验之中，测试数据构造一直是影响开发人员自测和测试人员测试质量的一个重要因素，开发人员疲于为测试或者产品体验构造特定场景所需的测试数据，而测试人员往往总因为测试数据不符合用例前置条件的要求，被迫等待开发人员构造数据，最终导致大量的沟通成本和时间成本。
为什么测试数据构造会如此麻烦？我认为主要还是业务本身的流程过长带来的问题，比如看一个典型的供应链商品采购仓储环节的流程： 在一个黑盒测试的场景下，假如测试人员需要针对“入库预约”环节进行测试，为了避免脏数据导致业务流程中断，最好的方式是从流程起点重新构造整套测试数据，然而在人手操作的情况下，这显然是难以完成的。
测试数据管理 我调研了国内外关于测试数据构造相关的一些讨论，发现了测试数据管理，英文 Test Data Management，为了逼格，可以简称“TDM”，这个术语的存在。关于测试数据管理，大家都在说它的成本有多高，但是至今却也没有找到相对通用的解决方案，更别说成熟的解决方案了。
测试数据管理的方案思考 按照测试数据管理的切入点以及执行的方式，我觉得可以归纳总结出以下几种形式：
复制生产环境数据：这种方式产生的数据，一般都满足严格的业务测试需求，数据的质量比较高，但是引入隐私安全和监管风险，且数据量比较大，存储成本高； 复制部分生产环境数据：这种方式降低存储成本，但是提高了维护成本，特别是关系型数据库，在所谓“部分”数据的前提下，需要针对不同库表指定不同的数据过滤方案； 复制生产数据，进行数据混淆处理：这种方式可以起到一定的隐私安全防护，但是类似复制生产环境数据的方案，需要针对不同库表指定不同的数据混淆规则，同时需要注意关系数据库的主键外键关系等； 执行接口调用，生成业务数据：这种方式在接口正常符合和逻辑正确的前提下，方便获得符合测试条件的数据。但是有维护开发成本，需要跟着接口设计调整，改动相对高频。另外特定场景，比如支付，需提供后门接口，以便测试脚本绕过支付等涉及第三方服务的业务，留下安全隐患。另外脚本不得不适配不同的接口协议和复杂接口认证流程等； 直接向数据库插入符合测试前置条件的新数据：自由度最高，数据完整度可控，能够获得隔离性最高的测试数据集。维护成本高，耦合技术方案存储层设计。管理成本高，需要一种高效组织和检索的方式以避免重复和凌乱的测试数据集。但是这种方式是相对稳定的方案。 我的方案 （待续……）
参考资料 What is Test Data Management? What is Test Data Management?</description></item><item><title>解决 Mac OS 下 MySQL 客户端连接 caching_sha2_password 插件加载失败问题</title><link>https://blog.hackerpie.com/posts/2021/fix-mysql-caching-sha2-password-problem/</link><pubDate>Sun, 20 Jun 2021 11:29:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2021/fix-mysql-caching-sha2-password-problem/</guid><description>背景 在开发我自己的 gofixtures 项目时，项目单测需要用到 MySQL，于是模仿 go-txdb 的方式，使用 docker 在本地起了 MySQL 容器。执行测试时，出现如下错误：
mysql: [Warning] Using a password on the command line interface can be insecure. ERROR 2059 (HY000): Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/mysql/lib/plugin/caching_sha2_password.so, 2): image not found 原因分析 结论：本地客户端版本过低，不支持服务器端版本的鉴权方式。
环境 客户端：
mysql Ver 14.14 Distrib 5.7.13, for osx10.11 (x86_64) using EditLine wrapper macOS 10.14.6 Mojave 服务器端：
version: 8.0.25 runtime environment: docker container 原因 MySQL 从 8.</description></item><item><title>《Paxos Made Simple》中文翻译：Paxos 如此简单</title><link>https://blog.hackerpie.com/posts/2020/paxos-made-simple-translation/</link><pubDate>Wed, 28 Oct 2020 19:48:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2020/paxos-made-simple-translation/</guid><description>写在前面 个人在学习理解 Paxos 算法的过程中，花了比较多的时间，从最开始直接查看中文博客资料，感觉都是看完不知所以然或者有很多疑问，于是决定死磕《Paxos Made Simple》论文原文。但是由于有些英文的意思我自己理解起来还是有点困惑，于是过程中遇到无法理解的内容，一方面是会翻阅前辈们已经写过的论文的翻译作为参考，二是在搜索引擎里就一些难以理解的点搜索中英文的讨论，以此解决自己心中的困惑。在磕磕碰碰中完成论文的阅读之后，仍有一些不尽透彻之处，加上个人认为此论文已有的翻译质量参差不齐，所以斗胆想通过翻译以及必要译注再次加深自己的理解，另外可能的话，也希望本次翻译能够帮助到未来可能会遇到和我一样困惑的人。
部分关键术语表 论文中有一些关键术语，我已经力求用词准确，并在论文中尽力保持术语翻译的一致性，目的是尽量充分传达论文本身用词的精准，建议读者可先仔细阅读此表。
原文术语 翻译中使用术语 译者注 value(s) 值 值可能比较抽象，觉得太抽象的读者建议理解为提案的“内容”亦可 learn 获知 有些文章译作“了解”或者“学习”，但是这里反复斟酌，还是觉得“获知”更贴切，目的性更强烈 propose 提议 chosen 选定 一个被选定的值，意味着一个被“一致”确认下来的值 agent 代理 依旧觉得翻译成“代理”过于字面化 proposer(s) 提议者 acceptor(s) 接受者 learner(s) 学习者 fail / failure 失效/故障 意味着系统已经完全不能工作 proposal 提案 accept 接受 number 编号 distinguished 特定的 文中用于形容某个经过选举而被选中的角色 翻译全文 Paxos 如此简单 2001年11月1日</description></item><item><title>谨防猴子补丁以及Python中排查技巧</title><link>https://blog.hackerpie.com/posts/2019/pay-attention-to-monkey-patch/</link><pubDate>Sun, 22 Sep 2019 19:48:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2019/pay-attention-to-monkey-patch/</guid><description>背景 前两天晚上线上系统突发故障，在立马打开线上错误日志之后，却只能得到一堆毫无意义的程序调用栈(traceback)的输出，于是团队成员陷入漫长而又抓瞎的问题排查过程中。问题很幸运地得到了解决，但是我一直想不明白为什么日志里打印的调用栈毫无意义，按照经验，它应该打印的是异常产生过程中的调用栈才是。在经过后续的源码分析和排查之后，我才发现其实是因为项目中一个老旧的代码使用了猴子补丁导致，这也是这篇文章想要讨论的内容。
什么是猴子补丁 猴子补丁是一种用来在运行时修改（增加、变更、删除等）系统软件行为的编程方式。在动态语言里有广泛的猴子补丁应用的影子，比如 Ruby 的打开类的特性支持运行时扩展类的定义甚至替换方法的实现，Python 的方法或者函数由于可以在运行时进行替换而使得猴子补丁的应用非常方便，其他像 JavaScript 语言同样可以应用猴子补丁。
猴子补丁是把双刃剑 猴子补丁以其灵活性，可以实现补丁代码和应用代码的完全分离，同时使得应用代码在调用方式上保持调用方式始终不变。 从应用代码的角度来看，它调用的就是某个模块的原始定义的方法或者函数；而从被调用的方法或者函数的角度来看，猴子补丁的存在对它是透明的存在，以下展示一个 Python 语言的 Demo：
我们从一个极简例子开始，向这个美好的世界问好：
def greet(): print(&amp;#34;Hello World!&amp;#34;) if __name__ == &amp;#34;__main__&amp;#34;: greet() 假如执行以上脚本，得到的结果是：
$ python demo.py Hello World! 这个很简单，接下来假如打一个猴子补丁：我们扩充原来的 greet 的行为，现在除了打印信息，还要打印下当前的时间：
from datetime import datetime def greet(): print(&amp;#34;Hello World!&amp;#34;) # monkey patch original_greet = greet def greet_with_time(): original_greet() print(datetime.now()) greet = greet_with_time # replace the implementation # monkey patch if __name__ == &amp;#34;__main__&amp;#34;: greet() # 这里的调用和原来没有变化 运行它，得到的结果是：</description></item><item><title>Mac OS 环境 Rails 6.0 下 webpack-dev-server wrong version 问题解决方案</title><link>https://blog.hackerpie.com/posts/2019/fix-webpack-dev-server-wrong-version-in-rails-6/</link><pubDate>Fri, 06 Sep 2019 15:18:30 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2019/fix-webpack-dev-server-wrong-version-in-rails-6/</guid><description>错误信息 昨天装上了 Ruby on Rails 6.0，满心欢喜初始化项目并且按照指引安装了 webpacker 之后，执行熟悉无比的 rails c 命令，却给了一个报错：
# 错误信息片段 yarn check v1.7.0 success Folder in sync. Done in 0.15s. yarn check v1.7.0 error &amp;quot;webpack-dev-server#yargs#cliui&amp;quot; is wrong version: expected &amp;quot;^4.0.0&amp;quot;, got &amp;quot;5.0.0&amp;quot; error &amp;quot;webpack-dev-server#yargs#yargs-parser&amp;quot; is wrong version: expected &amp;quot;^11.1.1&amp;quot;, got &amp;quot;13.1.1&amp;quot; error Found 2 errors. info Visit https://yarnpkg.com/en/docs/cli/check for documentation about this command. 解决方案 目前关于 Rails 6.0 相关的资料感觉不多，所幸找到了一篇日文版的帖子，成功解决了上边的问题：
$ brew upgrade yarn $ yarn upgrade 最终问题解决，又能愉快地前进了！
**注意：**我自己已经一年多没怎么开发 Rails 项目了，全栈开发那就更久远了。Rails 6.</description></item><item><title>pymysql 开启调试模式</title><link>https://blog.hackerpie.com/posts/2019/enable-debug-mode-of-pymysql/</link><pubDate>Sun, 18 Aug 2019 18:32:01 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2019/enable-debug-mode-of-pymysql/</guid><description>今天在排查线上一个奇怪的数据库连接问题，所以打开了 pymysql 的源码在阅读，发现 pymysql 在其 connections 模块里内置了一个 DEBUG 变量用于控制是否开启调试模式，是的话，会将当前连接的操作以及报文内容都打印到控制台。
使用方法 在你的服务器初始化代码里，加上对 DEBUG 的设置，比如：
import pymysql pymysql.install_as_MySQLdb() pymysql.connections.DEBUG = True # 这是我新加的一行 重启服务器后，访问相关接口，会看到标准输出里有类似下面的一些输出：</description></item><item><title>django 快速启动数据库客户端程序</title><link>https://blog.hackerpie.com/posts/2019/quick-launch-database-client-in-django/</link><pubDate>Thu, 15 Aug 2019 08:18:30 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2019/quick-launch-database-client-in-django/</guid><description>实际工作经历中，免不了有时候需要连接数据库进行问题排查分析的场景，之前一直习惯通过 mysql -uxxx -hxxxx -P1234 ... 这样的方式来启动命令行形式的 MySQL 数据库客户端程序，只是用起来比较麻烦，每次都要拷贝各个配置参数，还要记得不要在命令里显式打印密码。后来想起来在开发 Ruby on Rails 程序的时候，其提供了 rails dbconsole 的命令，可以方便直接启动对应的数据库客户端命令行程序，联想到 Django 理论上也有，所以找到了 python manage.py dbshell 这个命令，使用效果和自己手动敲 mysql 命令行是一样的，省去繁琐的参数设定步骤。
使用效果 用法 其用法可以直接查询命令行帮助文档：
# python manage.py dbshell -h Usage: manage.py dbshell [options] Runs the command-line client for specified database, or the default database if none is provided. Options: -v VERBOSITY, --verbosity=VERBOSITY Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings=SETTINGS The Python path to a settings module, e.</description></item><item><title>不严谨的不同语言下大 Excel 文件写入的性能比较</title><link>https://blog.hackerpie.com/posts/2019/general-excel-writor-benchmark-comparation-among-different-languages/</link><pubDate>Sat, 23 Mar 2019 21:11:06 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2019/general-excel-writor-benchmark-comparation-among-different-languages/</guid><description>背景 去年因为线上系统需要导出大量数据（大概是 11 万行）到 Excel，代码是 Python 2.7 写的，除去数据库查询耗时，整个的 Excel 文件生成也还要耗费几十秒的时间，这听起来真是一个非常夸张的事情。后来为其更换了号称性能表现最好的 pyexcelerate 库，性能确实有提升，但是仍是差强人意的在小几十秒。
昨天突发奇想，如果是换成其他语言，这个 excel 导出是否还需要这么长时间？于是经过一番试验之后，就有了今天的这篇文章。
**特别声明：**试验只是为了感官上做个简单对比，测试结果采集数据只考虑了耗时，没有考虑资源消耗等情况，需要严谨的性能对比的读者，可以放弃阅读了。
测试内容 使用不同的语言及其版本，测试各自完成包含 100,000 行 x 50 列单元格的 excel 文件的生成，对比其各自耗费时间，3次重复执行取其平均值后进行横向比较。
已经测试的语言及版本 Ruby 2.6 + axlsx 2.0.1 Python 2.7 + pyexcelerate 0.7.3 Python 3.6 + pyexcelerate 0.7.3 Go 1.10.1 + gooxml 0.8 测试代码 https://github.com/Martin91/excel-writors-benchmark
结果 结论 就这个测试场景来说：
Go 1.10.1 + gooxml 0.8 是最快的； 同样是 pyexcelerate 0.7.3，Python 2.7 性能优于 Python 3.6; Ruby 2.6 + axlsx 2.</description></item><item><title>利用 Postman Chrome app 和 Chrome 浏览器共享网站 cookie</title><link>https://blog.hackerpie.com/posts/2019/share-cookies-between-chrome-and-postman/</link><pubDate>Mon, 14 Jan 2019 09:22:19 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2019/share-cookies-between-chrome-and-postman/</guid><description>声明 文章内容已过期，Postman 桌面版已实现本文末尾的缺憾，可直接阅读官方博客。
背景 作为一个Web工程师，最熟悉的日常工作莫过于后台接口开发与联调测试，而在接口测试上，大家最喜爱的工具清单里，必然少不了 Postman 这一利器。然而，有时接口测试需要准备好登录态，或者其他状态数据，而这些数据往往就存在浏览器 Cookie 里边。结合本文介绍的工具，便可以无缝在 Postman Chrome app （为什么强调是 Postman Chrome app，文章末尾会说明）和 Chrome 浏览器之间共享 Cookie，而这个共享过程对用户是透明的。
工具清单 以下工具请自行安装，我只贴下官方的软件界面截图。
Chrome 浏览器 Postman Chrome app Postman Interceptor 使用步骤 以下我们以 Github 网站为例，演示下如何实现 Cookie 共享。
一、确认 Postman Interceptor 插件安装成功（如图所示） 二、启动 Postman，在右上角的卫星小图标那里开启 Chrome Interceptor 三、在 Chrome 浏览器里正常登陆 GitHub 网站（此步骤没什么好演示的 ╭(╯^╰)╮） 四、在 Postman Chrome app 中直接模拟请求通知接口 接口路径：https://github.com/notifications?_pjax=%23js-pjax-container 也就是说，这个时候，我们虽然没有对 Postman 做特殊的 Cookie 设置，但是它的请求的登录态都被服务器验证通过了，cookie 共享成功！
假如这个时候退出浏览器的登录态呢？ 我们先从 GitHub 退出登录，还是刚才的请求，这个时候的响应是： 是的，因为 Chrome 里已经退出登录，所以 Postman 这边也自然失去登录态了，说明两边 Cookie 是同步的。</description></item><item><title>记一次Redis数据库配置导致的连接数泄露的问题</title><link>https://blog.hackerpie.com/posts/2018/ji-yi-ci-redisshu-ju-ku-pei-zhi-dao-zhi-de-lian-jie-shu-xie-lou-de-wen-ti/</link><pubDate>Sat, 10 Feb 2018 20:35:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/2018/ji-yi-ci-redisshu-ju-ku-pei-zhi-dao-zhi-de-lian-jie-shu-xie-lou-de-wen-ti/</guid><description>问题背景 去年圣诞节当天，突然收到一个我经手过的项目的告警邮件，错误消息显示**“Redis::CommandError: ERR max number of clients reached”**。 什么情况？难道这个项目翻车了？第一反应是这台服务器运行着自建的 Redis 数据库，但是客户端只有同个内网的一个 Ruby on Rails 的应用，怎么会有连接数爆掉的可能？
理论连接数计算 老衲掐指一算：
sidekiq 客户端所需连接数: 对面 Rails 应用有 10 个 Unicorn 工作进程，每个unicorn进程初始化一个 sidekiq 客户端，一个 sidekiq 客户端默认连接池大小是 5，而且是懒惰策略，按需连接的，最大值是 10 x 5 = 50； 显式 Redis 连接: 程序代码里有一个 $redis 全局变量，初始化了一个 redis 连接，10个工作进程，也就是 10 个连接； sidekiq 服务端所需连接数: sidekiq server 端 concurrency 配置是 10，那么按照官方文档，另有加上 2 个连接，也就是12个连接； Rails cache 所需连接数: 按照redis-store gem 源码，默认连接池大小应该是 5，10个 unicorn 工作进程，按需连接，最大值是 10 x 5 = 50。 在不考虑其他可能还用到 Redis 连接的情况下，目前已知的最大 Redis 连接数需求是 122，这个数远小于 Redis 理论最大连接数啊，而且当时显示连接数到达上万！而且这个项目已经很少访问，压力极其小，不大可能会达到理论所需连接数啊！</description></item><item><title>解读 Rails: Migrations</title><link>https://blog.hackerpie.com/posts/archive/jie-du-rails-migrations/</link><pubDate>Sat, 14 Oct 2017 22:29:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/jie-du-rails-migrations/</guid><description>此文翻译自Reading Rails - Migrations，限于本人水平，翻译不当之处，敬请指教！
今天我们将会探讨一下 Rails 经常被忽视的可靠的工作伙伴 —— Migrator。它是如何搜寻你的 migrations 并且执行它们的呢？我们将再一次慢慢地挖掘 Rails 的源代码，并在此过程中慧海拾珠。
为了跟随本文的步骤，请使用qwandry打开相关的代码库，或者直接在Github上查看这些代码。
动身启程 在展开讨论之前，此处并无特殊准备要求。或许你已经创建好了项目所需要的但是仍是空的数据库。如果你执行 rake db:migrate，所有的未执行的 migrations 就会开始执行。让我们从查看 databases.rake 里的 Rake 任务的源码开始动起来：
desc &amp;#34;Migrate the database (options: VERSION=x, VERBOSE=false, SCOPE=blog).&amp;#34; task :migrate =&amp;gt; [:environment, :load_config] do ActiveRecord::Migration.verbose = ENV[&amp;#34;VERBOSE&amp;#34;] ? ENV[&amp;#34;VERBOSE&amp;#34;] == &amp;#34;true&amp;#34; : true ActiveRecord::Migrator.migrate(ActiveRecord::Migrator.migrations_paths, ENV[&amp;#34;VERSION&amp;#34;] ? ENV[&amp;#34;VERSION&amp;#34;].to_i : nil) #... end 虽然我们并不打算揭露 Rake 本身的工作机制，但是值得注意的是，执行 migrate 要求另外两个任务 [:environment, :load_config] 的首先执行。这能确保 Rails 的运行环境以及你的 database.yml 文件被加载进来。
上面的 rake 任务通过环境变量配置了 ActiveRecord::Migration 以及 ActiveRecord::Migrator。环境变量是一种非常有效的可用于向你的应用程序传递信息的方式。缺省地，诸如USER的很多（环境）变量都是已经设置好的，他们也可以在每个（终端）命令执行时单独设置。举个例子，如果你通过 VERBOSE=false rake db:migrate 调用了 Rake 任务，ENV[&amp;quot;VERBOSE&amp;quot;]的值就会是字符串&amp;quot;false&amp;quot;。</description></item><item><title>动态密码算法介绍与实现</title><link>https://blog.hackerpie.com/posts/archive/dong-tai-mi-ma-suan-fa-jie-shao-yu-shi-xian/</link><pubDate>Sat, 18 Feb 2017 09:54:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/dong-tai-mi-ma-suan-fa-jie-shao-yu-shi-xian/</guid><description>动态密码，亦称一次性密码（One Time Password, 简称 OTP），是一种高效简单又比较安全的密码生成算法，在我们的生活以及工作中随处可见，身为开发者，也或多或少在自己的业务系统中集成了二步验证机制，那么，技术运用，既要知其然，更要知其所以然，动态密码算法是怎样的？
读前指引 通过这篇文章，你可以了解以下知识： 动态密码的背景知识 动态密码的分类 不同动态密码的生成算法，HOTP 以及 TOTP HOTP 以及 TOTP 的简单的 Ruby 编程语言的实现 两类算法各自注意事项 限于篇幅，我不会讨论以下几点，有兴趣的同学可以参考我文章末尾给出的参考资料了解： 不同动态密码的安全性分析 计时动态密码如何确保有效期间内，密码不被二次使用 动态密码背景介绍 从我的角度理解，动态密码是指随着某一事件（密码被使用、一定的时间流逝等）的发生而重新生成的密码，因为动态密码本身最大优点是防重复执行攻击(replay attack)，它能很好地避免类似静态密码可能被暴力破解等的缺陷，现实运用中，一般采用“静态密码+动态密码”相结合的双因素认证，我们也称二步验证。
而动态密码其实很早就出现在我们的生活里了，在移动支付发展起来之前，网银是当时最为流行的在线支付渠道，当时银行为了确保大家的网银账号支付安全，都会给网银客户配发动态密码卡，比如中国银行电子口令卡（按时间差定时生成新密码，口令卡自带电池，可保证连续使用几年），或者工商银行的电子银行口令卡（网银支付网页每次生成不同的行列序号，用户根据指定行列组合刮开密码卡上的涂层获取密码，密码使用后失效），又或者银行强制要求的短信验证码，这些都可以纳入动态密码的范畴。
而随着移动互联网的发展以及移动设备的智能化的不断提高，设备间的同步能力大幅提升，以前依赖独立设备的动态密码生成技术很快演变成了手机上的动态密码生成软件，以手机软件的形式生成动态密码的方式极大提高了动态密码的便携性，一个用户一个手机就可以管理任意多个动态密码的生成，这也使得在网站上推动二步验证减少了很多阻力，因为以往客户可能因为使用口令卡太麻烦，而拒绝打开二步验证机制，从而让自己的账号暴露在风险之下。最为知名的动态密码生成软件，当属 Google 的 Authenticator APP。
动态密码算法探索之旅 动态密码的分类 一般来说，常见的动态密码有两类：
**计次使用：**计次使用的OTP产出后，可在不限时间内使用，知道下次成功使用后，计数器加 1，生成新的密码。用于实现计次使用动态密码的算法叫 HOTP，接下来会对这个算法展开介绍； **计时使用：**计时使用的OTP则可设定密码有效时间，从30秒到两分钟不等，而OTP在进行认证之后即废弃不用，下次认证必须使用新的密码。用于实现计时使用动态密码的算法叫 TOTP，接下来会对这个算法展开介绍。 在真正开展算法介绍之前，需要补充介绍的是：动态密码的基本认证原理是在认证双方共享密钥，也称种子密钥，并使用的同一个种子密钥对某一个事件计数、或时间值进行密码算法计算，使用的算法有对称算法、HASH、HMAC等。记住这一点，这个是所有动态密码算法实现的基础。
HOTP HOTP 算法，全称是“An HMAC-Based One-Time Password Algorithm”，是一种基于事件计数的一次性密码生成算法，详细的算法介绍可以查看 RFC 4226。其实算法本身非常简单，算法本身可以用两条简短的表达式描述：
HOTP(K,C) = Truncate(HMAC-SHA-1(K,C)) PWD(K,C,digit) = HOTP(K,C) mod 10^Digit
上式中：</description></item><item><title>周末到了，来段代码压压惊</title><link>https://blog.hackerpie.com/posts/archive/zhou-mo-dao-le-lai-duan-dai-ma-ya-ya-jing/</link><pubDate>Sat, 26 Nov 2016 21:43:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/zhou-mo-dao-le-lai-duan-dai-ma-ya-ya-jing/</guid><description>最近一段时间，写了两篇关于 sidekiq 的源码分析，但是一直想要补充的一段 sidekiq 里边的代码其实是挺有趣也挺逗的，所以这个星期就不要长篇大论的源码分析，来点轻松点的吧。
这个代码是这样的 o(╯□╰)o：
# https://github.com/mperham/sidekiq/blob/5ebd857e3020d55f5c701037c2d7bedf9a18e897/lib/sidekiq.rb#L51-L53 module Sidekiq # ... def self.❨╯°□°❩╯︵┻━┻ puts &amp;#34;Calm down, yo.&amp;#34; end ... end Haha, are you kidding me? 见过用特殊字符或者特殊语言文字做方法名的，但是用颜文字，我还是第一次见。但是别笑，本着工科男严谨与求知的精神，我全局搜索了下这个方法的调用，结果更搞笑的结果来了，这个方法根本就没有真实调用，但是相应的测试用例同样非常逗 2333333333！！！
describe &amp;#34;❨╯°□°❩╯︵┻━┻&amp;#34; do before { $stdout = StringIO.new } after { $stdout = STDOUT } it &amp;#34;allows angry developers to express their emotional constitution and remedies it&amp;#34; do Sidekiq.❨╯°□°❩╯︵┻━┻ assert_equal &amp;#34;Calm down, yo.\n&amp;#34;, $stdout.string end end 哈哈，这只是难道为了方便程序员怒火中烧的时候表达想掀桌的内心吗？
当然，这个问题其实早就有很多人发现了，Ruby China 上也有好多的讨论了。今天是个快乐周六，让我再从网络上搜罗多一些搞笑的代码吧，哈哈~~~
精彩段子时间 每一个在注释或者代码里藏段子的程序员上辈子都是折翼的逗逼，不信，你看！</description></item><item><title>Sidekiq 信号处理源码分析</title><link>https://blog.hackerpie.com/posts/archive/sidekiq-xin-hao-chu-li-yuan-ma-fen-xi/</link><pubDate>Sun, 20 Nov 2016 10:08:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/sidekiq-xin-hao-chu-li-yuan-ma-fen-xi/</guid><description>引言 在之前的文章《Sidekiq任务调度流程分析》中，我们一起仔细分析了 Sidekiq 是如何基于多线程完成队列任务处理以及调度的。我们在之前的分析里，看到了不管是 Sidekiq::Scheduled::Poller 还是 Sidekiq::Processor 的核心代码里，都会有一个由 @done 实例变量控制的循环体：
# https://github.com/mperham/sidekiq/blob/5ebd857e3020d55f5c701037c2d7bedf9a18e897/lib/sidekiq/scheduled.rb#L63-L73 def start @thread ||= safe_thread(&amp;#34;scheduler&amp;#34;) do initial_wait while !@done # 这是 poller 的循环控制 enqueue wait end Sidekiq.logger.info(&amp;#34;Scheduler exiting...&amp;#34;) end end # https://github.com/mperham/sidekiq/blob/5ebd857e3020d55f5c701037c2d7bedf9a18e897/lib/sidekiq/processor.rb#L66-L77 def run begin while !@done # 这是我们常说的 worker 循环控制 process_one end @mgr.processor_stopped(self) rescue Sidekiq::Shutdown @mgr.processor_stopped(self) rescue Exception =&amp;gt; ex @mgr.processor_died(self, ex) end end 也就是说，这些 @done 实例变量决定了 poller 线程跟 worker 线程是否循环执行？一旦 @done 被改为 true，那循环体就不再执行，线程自然也就是退出了。于是，单从这些代码，我们可以断定， Sidekiq 就是通过设置 @done 的值来通知一个线程安全退出（graceful exit）的。我们也知道，生产环境中，我们是通过发送信号的方式来告诉 sidekiq 退出或者进入静默(quiet)状态的，那么，这里的 @done 是怎么跟信号处理联系起来的呢？这些就是今天这篇文章的重点了！</description></item><item><title>sidekiq任务调度流程分析</title><link>https://blog.hackerpie.com/posts/archive/sidekiqren-wu-diao-du-liu-cheng-fen-xi/</link><pubDate>Sat, 29 Oct 2016 16:32:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/sidekiqren-wu-diao-du-liu-cheng-fen-xi/</guid><description>sidekiq是 Ruby 中一个非常优秀而且可靠的后台任务处理软件，其依赖 Redis 实现队列任务的增加、重试以及调度等。而 sidekiq 从启动到开始不断处理任务、定时任务以及失败任务的重试，都是如何调度的呢？遇到问题的时候，又该如何调优呢？
注意 今天的分析所参考的 sidekiq 的源码对应版本是 4.2.3； 今天所讨论的内容，将主要围绕任务调度过程进行分析，无关细节将不赘述，如有需要，请自行翻阅 sidekiq 源码； 文章内容真的很长，请做好心理准备。
你将了解到什么？ sidekiq 的任务调度机制：定时任务、重试任务的检查机制，队列任务的排队以及队列权重对处理优先级的影响； sidekiq 的中间件机制以及在此基础上实现的任务重试机制。 先抛结论 时序图 对于复杂的调用关系，我习惯用时序图帮助我理解其中各部分代码之间相互协作的关系（注意：为了避免太多细节造成阅读负担，我将参数传递以及返回值等冗杂过程去除了，只保留与任务调度相关的关键调用）： ![sidekiq 任务调度时序图](/images/medias/sidekiq job dispatcher.png)
人话 Sidekiq 整个任务调度过程中依赖几个不同角色的代码共同协作，其分工如下： 源码之旅 —— 启动 当我们在执行 sidekiq 时，源码中的 bin/sidekiq.rb 文件便是第一个开始执行的文件，让我们看看里边的主要代码：
# https://github.com/mperham/sidekiq/blob/5ebd857e3020d55f5c701037c2d7bedf9a18e897/bin/sidekiq#L9-L12 begin cli = Sidekiq::CLI.instance cli.parse cli.run # &amp;lt;===== 这边走 # ... 紧靠 begin 后边的两行代码首先创建 Sidekiq::CLI 类的一个实例，接着调用实例方法 #parse 解析 sidekiq 的配置参数，其中包括队列的配置、worker 数量的配置等，在此不展开了。接着实例方法 #run 将带着我们继续往下走，让我们继续看 lib/sidekiq/cli.rb 里边的代码：</description></item><item><title>嘿，小心你的双等号==</title><link>https://blog.hackerpie.com/posts/archive/pay-attention-to-your-double-equals/</link><pubDate>Sun, 17 Jan 2016 01:40:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/pay-attention-to-your-double-equals/</guid><description>前两天在写代码的时候，突然收到警告说项目代码中存在 XSS 漏洞，遂立即根据报告的 URL 排查页面代码，虽然很快就修复了，而且同样问题的讨论两年前就有了，看RubyChina: 别用 raw 和 html_safe，一般来说相对有经验的老鸟也应该都知道这个点，但是还是觉得有必要写出来，再次提醒一下其他小伙伴，避免踩坑。
问题根源 其中，在找到的漏洞出现的地方，都存在类似以下这样的 slim 代码：
input class=&amp;#39;xxx&amp;#39; value==params[:account] 问题就出在双等号 == 上，因为在 slim 跟 ERB 模板（其他模板比如 HAML 之类的就不清楚了）中，双等号其实是 Rails 的 raw 这个 helper 方法的缩写，参考链接：
To insert something verbatim use the raw helper rather than calling html_safe:
&amp;lt;%= raw @cms.current_template %&amp;gt; &amp;lt;%# inserts @cms.current_template as is %&amp;gt; or, equivalently, use &amp;lt;%==:
&amp;lt;%== @cms.current_template %&amp;gt; &amp;lt;%# inserts @cms.current_template as is %&amp;gt; 也就是说上面的代码等同于：</description></item><item><title>谨防 ActiveSupport::Cache::Store 缓存 nil 值</title><link>https://blog.hackerpie.com/posts/archive/jin-fang-activesupport-cache-store-huan-cun-nil-zhi/</link><pubDate>Fri, 30 Oct 2015 20:48:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/jin-fang-activesupport-cache-store-huan-cun-nil-zhi/</guid><description>Rails 中的 active_support 组件主要基于 Rails 需要提供了很多非常有用的基础工具以及对 Ruby 内置类进行扩展。其中的 cache 模块主要提供了 Rails 中底层缓存的定义以及简单实现。今天要跟大家探讨的是之前在使用此模块所遇到的一个坑，有兴趣学习其基本用法的可以点击以下两个链接：
Rails Guides: ActiveSupport::Cache::Store Rails API: ActiveSupport::Cache::Store 从 ActiveSupport::Cache::Store#fetch 聊起 之前在实现一个需要从外部服务请求数据的功能时，处于性能考虑，我在代码中使用了缓存，并且设置缓存失效时间为 7 天，示例代码如下：
def read_external_service(params) # 这段代码稍微解释下： # 当缓存命中时，则直接读取缓存，如果无期待缓存，则通过 HTTP 向外请求结果，并且将结果 # 缓存下来，这样子，当下次继续调用时，则可直接返回缓存内容，而无需重复向外请求 # Rails.cache.fetch &amp;#39;example_cache_key_here&amp;#39;, expires_in: 7.days do response = HTTParty.get &amp;#39;https://example.com/example/request/path&amp;#39; JSON.parse(response.body)[&amp;#34;data&amp;#34;] end end 上面的代码其实不复杂，核心代码就是使用了 ActiveSupport::Cache::Store#fetch 方法。
一切都很正常地运行着，直到有一天，线上系统不断报警，出错原因就是这段代码总是返回 nil ，而调用者又因为没有判断 nil 值，就会出现 undefined method 'xxx' for nil:NilClass 错误。在 debug 时，我尝试了直接调用外部服务接口，发现请求都有正确返回数据，不可能返回 nil 啊，难道是缓存了 nil 值？下面就直接通过代码验证一下！
[1] pry(main)&amp;gt; require &amp;#39;active_support&amp;#39; =&amp;gt; true [2] pry(main)&amp;gt; cache = ActiveSupport::Cache::MemoryStore.</description></item><item><title>Apdex——衡量服务器性能的标准</title><link>https://blog.hackerpie.com/posts/archive/the-correct-way-to-metric-server-response-time/</link><pubDate>Thu, 30 Jul 2015 12:12:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/the-correct-way-to-metric-server-response-time/</guid><description>日常工作中，我们总是习惯于通过量化的标准去衡量我们对事物的评价，比如美食点评的星级、酒店的星级、每个个人的信用评分等等。而作为一个 Web 工程师，我们也总是在意于我们网站的性能，因为网站的性能会最直接地影响用户的体验。今天要介绍的就是一种同样能够帮助工程师对应用性能进行量化评估的标准 —— Apdex 。
Apdex 全称是 Application Performance Index，是由 Apdex 联盟开放的用于评估应用性能的工业标准。Apdex 联盟起源于 2004 年，由 Peter Sevcik发起。Apdex 标准从用户的角度出发，将对应用响应时间的表现，转为用户对于应用性能的可量化为范围为 0-1 的满意度评价。
术语 Apdex 定义了应用响应时间的最优门槛为T，另外根据应用响应时间结合 T 定义了三种不同的性能表现：
Satisfied（满意）：应用响应时间低于或等于 T（T 由性能评估人员根据预期性能要求确定），比如 T 为 1.5s，则一个耗时 1s 的响应结果则可以认为是 satisfied 的。 Tolerating（可容忍）：应用响应时间大于 T，但同时小于或等于 4T。假设应用设定的 T 值为 1s，则 4 * 1 = 4 秒极为应用响应时间的容忍上限。 Frustrated（烦躁期）：应用响应时间大于 4T。 公式 Apdext= (Satisfied Count + Tolerating Count / 2) / Total Samples其中 Satisfied Count 就是指定采样时间内响应时间满足 Satisfied 要求的应用响应次数；而 Tolerating Count 就是指定采样时间内响应时间满足 Tolerating 要求的应用响应次数；最后的 Total Samples 就是总的采样次数总数。从公式可以看出，应用的 Apdex 得分与采样持续时间无关，与目标响应时间 T 相关（在采用总数固定的情况下，T 通过影响 Satisfied Count以及 Tolerating Count的值间接影响最终的得分）。</description></item><item><title>申请以及集成 Stripe 的 Alipay 支付方案</title><link>https://blog.hackerpie.com/posts/archive/shen-qing-yi-ji-ji-cheng-stripe-de-alipay-zhi-fu-fang-an/</link><pubDate>Sat, 28 Mar 2015 12:12:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/shen-qing-yi-ji-ji-cheng-stripe-de-alipay-zhi-fu-fang-an/</guid><description>最近在一个项目需要支持人民币支付，并且客户要求希望能够收完款后的结算是用美元，所以就想到了去年 Stripe 宣布已经跟支付宝达成合作意向，所以经过一番咨询跟集成，终于把 Stripe 集成进来，并且启用了支付宝收款。这篇文章介绍功能申请以及集成的完整过程。
功能申请 注册 Stripe 账号； 加入 beta 用户组，电子邮箱跟注册的 Stripe 账号保持一致； 联系 Stripe 员工
发送邮件到 support@stripe.com，声明你需要在你的 Stripe 账号中启用 Alipay 的支付功能，并且提供你的 Stripe 账号。然后，等待回复就是，一般当天都能收到回复的。 集成 0. 时序图(可结合后边代码一起理解) 1. 引入 stripe.js 以及初始化脚本 假设支付页面上有个开始支付按钮，其 html 代码为:
&amp;lt;button id=&amp;#39;pay&amp;#39;&amp;gt;支付&amp;lt;/button&amp;gt; 请在 html 代码里合适的地方（比如&amp;lt;body&amp;gt;标签的底部）加载 stripe.js：
&amp;lt;script src=&amp;#34;https://checkout.stripe.com/checkout.js&amp;#34;&amp;gt;&amp;lt;/script&amp;gt; 在脚本中初始化 stripe.js，并且注册支付按钮的事件监听函数：
$(function(){ var stripeHandler = StripeCheckout.configure({ key: &amp;#39;pk_test_xxxxxxxxxxxxxxxxxxxxxxxx&amp;#39;, // 可以查看 https://dashboard.stripe.com/account/apikeys image: &amp;#39;https://placehold.it/200x200&amp;#39;, // 显示在支付对话框的图片，可自己指定 alipay: true, // 启用支付宝支付 token: function(token){ // 用户填写完资料并且 Stripe 校验成功后的回调函数 // 此时应该提交 token.</description></item><item><title>How do I fix Passenger application startup problem</title><link>https://blog.hackerpie.com/posts/archive/how-do-i-fix-passenger-application-startup-problem/</link><pubDate>Sat, 10 Jan 2015 01:54:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/how-do-i-fix-passenger-application-startup-problem/</guid><description>Recent days I was working on deploying one of my Rails project on a complete new VPS. I had operated according to my experience for deploying sites before, but at the last step, after I have deployed the site, it always raised error message &amp;ldquo;An error occurred while starting up the preloader: it did not write a startup response in time.&amp;rdquo; when I try to visit the site. But, thanks to much hard work and retry, I found the source of the problem and finally fix it.</description></item><item><title>在 coding.net 上部署 Jekyll 博客</title><link>https://blog.hackerpie.com/posts/archive/zai-coding-dot-netshang-bu-shu-jekyllbo-ke/</link><pubDate>Sat, 20 Dec 2014 12:59:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/zai-coding-dot-netshang-bu-shu-jekyllbo-ke/</guid><description>自从 coding 推出 PaaS 演示平台以及开放自定义域名之后，很多人开始尝试在 coding 上部署自己的博客，其中就有 jekyll，coding 上就有官方推荐的 jekyll-demo。但是因为这个 Demo 的 README 文档中只是简单介绍配置步骤而已，没有详细介绍原理以及灵活配置的地方，我在参照着迁移 jekyll 博客的过程中也遇到一些问题。现在写下文章，希望能够把原理理清楚。
**声明：**这篇文章主要是对原来的 Demo 的几个主要思路做一个补充说明，而并非 coding 演示平台使用操作的详细教程，所以在有些细节上不一定覆盖到，建议最终的部署代码需要以官方推荐的 repo 里的代码为主。
基本原理 因为 Coding 提供的演示平台是通用的 PaaS 平台，并非类似 Github 或者 Gitcafe 的 Pages 服务，所以 jekyll 部署到演示平台需要解决三个问题：
1. 运行问题，blog 需要以常规 Web 程序的方式运行；
2. 启动脚本，部署完成后自动启动服务器；
3. 自动更新，blog 内容更新 push 后能够自动生成新的页面。
第一个问题我们可以通过 rack-jekyll 解决；第二个问题通过 Coding 约定的 Procfile 文件解决；第三个问题我们通过 Coding 的 Webhook 结合脚本解决。
1. 将 Jekyll 博客变为一个在线运行的 Rack 程序 Jekyll 原本是一个用于生成静态博客站点的框架，但是为了能够在 coding 演示平台上直接运行 Jekyll 博客，我们需要一个能够在 Unicorn 服务器上运行 Jekyll 的方法。通过原来 coding 提供的 Demo，找到了一个叫 rack-jekyll 的工具。</description></item><item><title>RAILS中利用YAML文件完成数据对接</title><link>https://blog.hackerpie.com/posts/archive/railszhong-jie-he-yamlwen-jian-wan-cheng-shu-ju-dui-jie/</link><pubDate>Wed, 12 Nov 2014 20:15:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/railszhong-jie-he-yamlwen-jian-wan-cheng-shu-ju-dui-jie/</guid><description>最近在做的Ruby on Rails项目中，需要将远程数据库中的数据对接到项目数据库中，但是远程的数据不仅数据表名跟字段命名奇葩，数据结构本身跟项目数据结构出入比较大，在数据导入过程中代码经历了几次重构，最后使用了YAML文件解决了基本数据1对接的问题。在此写一篇博文，我会尽量重现一路过来的代码变更，算是分享一下我的思考过程，也算是祭奠一下自己的苦逼岁月。
假设以及数据结构预览 因为远程数据库服务器为Oracle Server，我在项目中使用到了Sequel这个gem用于连接数据库以及数据查询，因为数据库连接的内容不是本文的重点，故后续代码直接用remote_database表示数据库连接，而根据Sequel的用法，我们可以直接使用remote_database[table_name]连接到具体的表。
本次需要从远程数据库中导入的基本数据主要有学生信息表（包含班级名称）、老师信息表以及专业信息表，相应地，项目中（以下称为“本地”）也已经创建好了对应的model。其中学生信息表的表名以及部分数据字段的从本地到远程的映射关系如表所示：
表名或字段名 本地 远程 表名 students XSJBXX 姓名 name XM 学号 number XH 年级 grade NJ 班级 belongs_to :klass BJMC(班级名称) 老师信息表的表名以及部分数据字段的映射关系为：
表名或字段名 本地 远程 表名 teachers JZGJBXX 姓名 name XM 职称 title ZC 证件号码 id_number ZJHM 数据对接第一版：属性方法显式赋值 第一个导入的数据表是学生的信息表，在最开始的时候，因为只需要考虑一张单独的表，所以代码写得简单粗暴，基本过程就是：根据需要的信息，查询对应的远程数据字段，然后使用属性方法赋值，最后保存接入的数据。对接方法的部分相关代码示例（为了方便阅读以及保护项目敏感信息，本文对项目中原有代码进行了缩减以及修改）：</description></item><item><title>sublime text 2基于语法的配置文件</title><link>https://blog.hackerpie.com/posts/archive/sublime-text-2ji-yu-yu-fa-de-pei-zhi-wen-jian/</link><pubDate>Sun, 03 Aug 2014 23:13:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/sublime-text-2ji-yu-yu-fa-de-pei-zhi-wen-jian/</guid><description>最近在学习Python编程语言，但是遇到一个小小的问题，就是原来Ruby的编码规范是用2个空格缩进的，所以以前在Sublime的全局用户配置中设置了&amp;quot;tab_size&amp;quot;: 2，所以在编辑Python文件的时候就每次都要从菜单中设置tab_size的大小为4。后来经过搜索，发现Sublime Text 2实际上是支持语法特定的配置的，具体的步骤是：
先打开一个Python代码文件，或者是设置当前文件的语法为&amp;quot;Python&amp;quot;；
点击菜单栏中的“Sublime Text 2 -&amp;gt; Preferences -&amp;gt; Settings - More -&amp;gt; Syntax Specific - User”；
可以看到打开了一个名字为&amp;quot;Python.sublime-setting&amp;quot;的文件，如果打开的文件的名字不是Python，请回头检查第一步。
在打开的特定语言的配置文件中，直接设置:
{ &amp;quot;tab_size&amp;quot;: 4, &amp;quot;translate_tabs_to_spaces&amp;quot;: true } 以上的设置会默认覆盖全局配置。
以上步骤参考自今日技巧：Sublime Text 2语法缩进配置和ipa文件生成。</description></item><item><title>Spree 2.3.0已经发布</title><link>https://blog.hackerpie.com/posts/archive/spree-2-dot-3-0yi-jing-fa-bu/</link><pubDate>Mon, 28 Jul 2014 21:32:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/spree-2-dot-3-0yi-jing-fa-bu/</guid><description>**声明：**原文来自Spree官方博客Spree 2.3.0 Released，原文发布日期是2014-06-30，本文仅作翻译。
简要介绍：Spree是一个基于Ruby on Rails开发的开源在线商城框架，提供了从商品展示购买、下单支付到库存管理以及订单管理等一系列基本功能，并且支持通过第三方扩展的形式定制或者扩展框架的功能，最新版本的Spree已经支持最新的Rails版本。
Spree 2.3的最新更改已经加入对Rails 4.1的支持，提供了更好的配置项的存储，更好的针对多店铺的支持，以及更好的游客追踪。Spree 2.3的发布，有赖于总的97位贡献者以及他们总的700多个的commit记录。现在，我们非常兴奋地宣布：Spree 2.3发布了！
Rails 4.1 的支持 现在，Rails 4.1已经得到了Spree 2.3的支持。如果你希望基于Rails 4.1进行开发，那么Spree 2.3就是特为你准备的发布版。
基于序列化记录的配置项 现在，所有的配置项存储在一个记录上，而不是存储在spree_preferences表中。这意味着，为了获取一个配置项，比如价格的计算器配置，就会触发一个数据库查询，所查询的那一行记录有一个包含了所有配置信息的命名为preferences的列。
而在此之前，对于每一个配置记录本身，可能都会有一个单独的数据库调用，而在查询到所请求的配置项之后，还是有可能会有任意数目的数据库调用产生。而现在，我们总的只需要调用一次，这意味着程序本身将会有一些速度上的提升。
更好的多店铺支持 我们已经添加了一个名为Spree::Store的model，用于支持基本的多店铺/多域名的站点。其在spree-multi-domain这个扩展的基础上提供了针对多店铺/多域名的基本框架。一些原有的配置项被转移到了这个model上，以此实现根据具体的店铺提供不同配置值：
Spree::Config[:site_name] 迁移到了 name Spree::Config[:site_url] 迁移到了 url Spree::Config[:default_meta_description] 迁移到了 meta_description Spree::Config[:default_meta_keywords] 迁移到了 meta_keywords Spree::Config[:default_seo_title] 迁移到了 seo_title 一个数据库迁移文件将会负责把这些原有的配置项转移到一个新的默认的store实例上。
一个新的名为ControllerHelpers::Store的Concern提供了一个current_storehelper，可以在请求的域名的基础上，通过它获取当前店铺。
更好的游客追踪 现在，我们用了一个签名的cookie在浏览器中存储游客的唯一的token。通过它允许关闭了浏览器的顾客可以在再次访问时继续完成他们的购物流程。更重要的是，这也帮助作为商店主人的你方便地识别游客的订单。由于我们在访客来访时都会设置cookies.signed[:guest_token]，所以除了订单，或许你可以把cookie用于其他用途。
举个实际例子，如果游客需要收藏一个商品，你可以创建一个用于记录收藏记录的model，然后把cookies.signed[:guest_token]赋值给这个model中的token字段。这将有助于你分析当前用户在此之前的订单以及收藏记录，这对于商品推荐将是非常有用的。
总结 你可以从Github上浏览更详细的变更列表。</description></item><item><title>Run a shell script auto-matically when entering/cd a directory</title><link>https://blog.hackerpie.com/posts/archive/run-a-shell-script-auto-matically-when-entering-a-directory/</link><pubDate>Sun, 04 May 2014 10:32:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/run-a-shell-script-auto-matically-when-entering-a-directory/</guid><description>I don&amp;rsquo;t know if it is common that you need to run some shell scripts which are used under only some directories, such as, one of your Rails projects.
Today I find that I always need to run rspec command with a SPEC option, which specifies spec files to be run. In short, everytime I should type the following command in my terminal:
rake spec SPEC=spec/lib/ It is convenient to run this command as an alias, but I don&amp;rsquo;t want to write this alias into the ~/.</description></item><item><title>Gemfile指定gem来源的四种方式</title><link>https://blog.hackerpie.com/posts/archive/gemfilezhi-ding-gemlai-yuan-de-si-chong-fang-shi/</link><pubDate>Thu, 03 Apr 2014 00:30:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/gemfilezhi-ding-gemlai-yuan-de-si-chong-fang-shi/</guid><description>Gemfile的作用无非就是告诉bundler你的项目具体都需要哪些gem，这些gem都需要哪些版本，以及从哪获取这些gem。其实你的问题应该就是跟第三点有关，总的来说，gem的来源可以有四种：
1. 从镜像源安装 这个是最直接的，通过这种方式指定的gem，bundler会从文件开头的source中去查找这个gem：
source &amp;#39;https://rubygems.org&amp;#39; gem &amp;#39;rails&amp;#39; # this gem will be installed from https://rubygems.org 2. 从git代码库安装 通过在gem方法（Gemfile实际上就是一个ruby的代码文件）中指定git参数，可以使bundler从指定的远程代码库上拉取代码，比如：
# nokogiri will be installed from git://github.com/tenderlove/nokogiri.git gem &amp;#39;nokogiri&amp;#39;, :git =&amp;gt; &amp;#39;git://github.com/tenderlove/nokogiri.git&amp;#39; 3. 从github安装 上面第2种方法只是针对所有合法的git代码库（不仅仅是github，也可以是你自己的一个git服务器上一个代码库）而言，而如果你所需要的库来自于github，则可以通过更方便的github参数实现目标:
gem &amp;#39;nokogiri&amp;#39;, :github =&amp;gt; &amp;#39;tenderlove/nokogiri&amp;#39; 可以看到，只要指定了author/repo_name的形式，bundler就能自动从github上获取你所需要的gem了。 注意: 第2跟第3种方式还都可以通过branch参数指定你所需要的代码分支，比如：
gem &amp;#39;refinerycms&amp;#39;, github: &amp;#39;refinery/refinerycms&amp;#39;, branch: &amp;#39;master&amp;#39; 4. 从文件系统中安装 假如你有一个已经放在项目目录中（其实可以是任何地方）的gem，则可以通过path参数指定所需的gem在文件系统中的位置，比如：
gem &amp;#34;rails&amp;#34;, :path =&amp;gt; &amp;#34;vendor/rails&amp;#34; bundler将会根据path指定的路径去查找并且安装gem。
最后说一下 最后顺便说下我的一点体会，一般像这种gem来自于项目目录下的情况，大多是因为项目中用到了一些提供扩展机制的框架，比如Spree以及Refinery，这两者生成的扩展或者子Engine都是以gem的形式放在vendor或者lib目录下，然后从Gemfile里边进行指定，比如我的一个项目中的一个实例：
gem &amp;#39;refinerycms-factories&amp;#39;, :path =&amp;gt; &amp;#39;vendor/extensions&amp;#39; refinerycms-factories是我用Refinery的generator生成的一个子engine，默认放在verdor/extensions目录下。
另一种可能比较常见的情况就是你用到了某个可能不再维护的gem，由于对源代码的改动较大，所以你干脆把这个gem的源代码下载到本地项目目录下，然后直接进行修改，最后通过path去安装。
参考资料 关于Gemfiile的更多资料，请自行猛戳： Bundler homepage Gemfile manual page</description></item><item><title>Ruby中Hash的7个日常使用范例</title><link>https://blog.hackerpie.com/posts/archive/rubyzhong-hashde-7ge-ri-chang-shi-yong-fan-li/</link><pubDate>Tue, 25 Mar 2014 20:05:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/rubyzhong-hashde-7ge-ri-chang-shi-yong-fan-li/</guid><description>此文翻译自7 daily use cases of Ruby Hash，限于本人水平，翻译不当之处，敬请指教！
每一天，你都需要跟Hash相处。创建一个新的Hash或者是通过它的某一个键去检索其中的元素这样的工作，都是常见也是非常简单的。但是当你需要合并两个嵌套的Hash或者是从某一个Hash里边过滤某些键，你可能需要考虑得多一点。通过完整的文档，你可以找到对Hash中的每一个方法的充分解释。但是由于文档不是面向应用场景的，你可能没法很快找到你的解决方案。在下面，我分享了我日常中经常遇到的Hash中的7个常用场景，希望它们对你有用。
1. 如何将一个JSON转换为一个Hash？ 假设你刚刚接收到一个用JSON表示的Twitter账号的资料信息：
data = &amp;#39;{ &amp;#34;name&amp;#34;: &amp;#34;Aaron Patterson&amp;#34;, &amp;#34;screen_name&amp;#34;: &amp;#34;tenderlove&amp;#34;, &amp;#34;location&amp;#34;: &amp;#34;Seattle, WA&amp;#34; }&amp;#39; 你希望能够将它转化为一个Hash,这样会更方便你进行对数据的操作：
require &amp;#39;json&amp;#39; profile = JSON.parse(data) ** 在IRB中的输出结果：**
=&amp;gt; { &amp;#34;name&amp;#34;=&amp;gt;&amp;#34;Aaron Patterson&amp;#34;, &amp;#34;screen_name&amp;#34;=&amp;gt;&amp;#34;tenderlove&amp;#34;, &amp;#34;location&amp;#34;=&amp;gt;&amp;#34;Seattle, WA&amp;#34; } 查看文档：JSON#parse
2. 如何将一个Hash转换为一个JSON？ 在你的web应用程序中，你需要追踪当前星期每一天新注册用户的数量：
signups_of_the_week = { monday: 2, tuesday: 3, wednesday: 4, thursday: 20, friday: 5, saturday: 2, sunday: 5 } 你可以通过API的方式把它们以JSON格式提供给客户端：
require &amp;#39;json&amp;#39; signups_of_the_week.to_json ** 在IRB中的输出结果：**
=&amp;gt; &amp;#34;{\&amp;#34;monday\&amp;#34;:2,\&amp;#34;tuesday\&amp;#34;:3,\&amp;#34;wednesday\&amp;#34;:4,\&amp;#34;thursday\&amp;#34;:20,\&amp;#34;friday\&amp;#34;:5,\&amp;#34;saturday\&amp;#34;:2,\&amp;#34;sunday\&amp;#34;:5}&amp;#34; 查看文档：JSON#generate 边注：JSON#pretty_generate对于更好的打印以及调试非常有用。</description></item><item><title>注意Rake Task中invoke方法跟execute方法的不同</title><link>https://blog.hackerpie.com/posts/archive/zhu-yi-rake-taskzhong-invokegen-executefang-fa-de-bu-tong/</link><pubDate>Fri, 21 Mar 2014 01:18:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/zhu-yi-rake-taskzhong-invokegen-executefang-fa-de-bu-tong/</guid><description>平时如果跟Rake Task有过接触的同学都会知道，当我们需要在一个Task里边调用另一个Task的时候，我们可以使用Rake::Task['task_name'].invoke的方式。但是在今天的实践中，才知道Rake::Task#invoke在默认情况下在整个运行过程中将只会被调用一次而已。话不多说，动手演示：
准备一个say hello的task，代码：
# lib/tasks/demo.rake namespace :demo do desc &amp;#34;Print &amp;#39;Hello&amp;#39; string&amp;#34; task :say_hello do puts &amp;#34;Hello, World!&amp;#34; end end 接下来在命令行中执行rake task:
$ rake demo:say_hello =&amp;gt; Hello, World! 假设我们一个循环，需要调用上边的task共5次，那么我们可能会这么写：
namespace :demo do # .... desc &amp;#34;Print &amp;#39;Hello, World!&amp;#39; five times&amp;#34; task :say_five_hello do 5.times do Rake::Task[&amp;#39;demo:say_hello&amp;#39;].invoke end end end Ok, 让我们尝试着运行这个say_five_hello的task，是不是真的会打印5次&amp;rsquo;Hello, World!&amp;lsquo;呢?
$ rake demo:say_five_hello =&amp;gt; Hello, World! 结果就是，&amp;lsquo;Hello, World!&amp;lsquo;只打印了一次，也就是说，我们的Rake::Task['demo:say_hello']只被运行了一次。
经过搜索，从StackOverflow找到了这个问题的相关描述，详见：How do I execute Rake tasks with arguments multiple times?</description></item><item><title>属性方法</title><link>https://blog.hackerpie.com/posts/archive/shu-xing-fang-fa/</link><pubDate>Wed, 12 Mar 2014 20:58:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/shu-xing-fang-fa/</guid><description>此文翻译自Reading Rails - Attribute Methods，限于本人水平，翻译不当之处，敬请指教！
在我们上一篇的探讨中，我们已经看到了Rails在跟踪属性变更中使用到的属性方法（attribute methods）。有三种类型的属性方法：前缀式（prefix）、后缀式（suffix）以及固定词缀式（ affix）。为了表述简洁，我们将只关注类似attribute_method_suffix这样的后缀式属性方法，并且特别关注它是如何帮助我们实现类似name这样的模型属性以及对应生成的类似name_changed?这样的方法的。
如果需要跟着我的步骤走，请使用qwandry打开每一个相关的代码库，或者直接从github查看源码即可。
声明（Declarations） 属性方法是Rails中众多使用了元编程技术的案例之一。在元编程中，我们编写可以编写代码的代码。举例来说，attribute_method_suffix后缀式方法是一个为每个属性都定义了一个helper方法的方法。在之前的讨论中，ActiveModel使用这种方式为您的每一个属性都定义了一个_changed?方法(提示： 命令行中键入qw activemodel查看代码)：
module Dirty extend ActiveSupport::Concern include ActiveModel::AttributeMethods included do attribute_method_suffix &amp;#39;_changed?&amp;#39;, &amp;#39;_change&amp;#39;, &amp;#39;_will_change!&amp;#39;, &amp;#39;_was&amp;#39; #... 让我们打开ActiveModel库中的attribute_methods.rb文件，并且看一下到底发生了什么事情。
def attribute_method_suffix(*suffixes) self.attribute_method_matchers += suffixes.map! do |suffix| AttributeMethodMatcher.new suffix: suffix end #... end 当你调用attribute_method_suffix方法的时候，每一个后缀都通过map!方法转换为一个AttributeMethodMatcher对象。这些对象会被存储在attribute_method_matchers中。如果你重新看一下这个module的顶部，你会发现attribute_method_matchers是在每一个包含此module的类中使用class_attribute定义的方法：
module AttributeMethods extend ActiveSupport::Concern included do class_attribute :attribute_aliases, :attribute_method_matchers, instance_writer: false #... class_attribute方法帮助你在类上定义属性。你可以这样在你自己的代码中这样使用：
class Person class_attribute :database #... end class Employee &amp;lt; Person end Person.database = Sql.new(:host=&amp;gt;&amp;#39;localhost&amp;#39;) Employee.database #=&amp;gt; &amp;lt;Sql:host=&amp;#39;localhost&amp;#39;&amp;gt; Ruby中并没有class_attribute的内置实现，它是在ActiveSupport(提示:命令行中键入qw activesupport查看代码)中定义的方法。如果你对此比较好奇，可以简单看下attribute.</description></item><item><title>跟踪model中属性（值）的变更</title><link>https://blog.hackerpie.com/posts/archive/zhui-zong-bian-geng/</link><pubDate>Fri, 07 Mar 2014 13:02:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/zhui-zong-bian-geng/</guid><description>此文翻译自Reading Rails - Change Tracking，限于本人水平，翻译不当之处，敬请指教！
我们今天来看看Rails是如何追踪model里边属性的变更的。
person = Person.find(8) person.name = &amp;#34;Mortimer&amp;#34; person.name_changed? #=&amp;gt; true person.name_was #=&amp;gt; &amp;#34;Horton&amp;#34; person.changes #=&amp;gt; {&amp;#34;name&amp;#34;=&amp;gt;[&amp;#34;Horton&amp;#34;,&amp;#34;Mortimer&amp;#34;]} person.save! person.changes #=&amp;gt; {} name_changed?方法是从哪来的呢？变更又是如何被创建的？让我们顺着这个场景，看看这一切背后的秘密。
如果需要跟着我的步骤走，请使用qwandry打开每一个相关的代码库，或者直接从github查看源码即可。
ActiveModel 当你想探寻ActiveRecord里边的功能时，你应该首先了解ActiveModel。ActiveModel（提示： 命令行中键入qw activemodel查看代码）定义了没有与数据库捆绑的逻辑。我们将从dirty.rb文件开始。在这个模块最开始的地方，代码调用了attribute_method_suffix：
module Dirty extend ActiveSupport::Concern include ActiveModel::AttributeMethods included do attribute_method_suffix &amp;#39;_changed?&amp;#39;, &amp;#39;_change&amp;#39;, &amp;#39;_will_change!&amp;#39;, &amp;#39;_was&amp;#39; #... attribute_method_suffix定义了定制的属性读写器。这主要用来告诉Rails将一些带有类似_changed?后缀的调用分发到特定的处理器方法上。为了看看它们是如何实现的，请向下滚动代码，并且找到def attribute_changed?：
def attribute_changed?(attr) changed_attributes.include?(attr) end 我们将会在另外的一篇文章中再着重介绍如何连接这些方法的细节，当你调用一个类似name_changed?的方法时，Rails将会把&amp;quot;name&amp;quot;作为参数attr传给上述方法。往回看一点点，你会发现changed_attributes只是一个包含了从属性名到旧的属性值的映射的Hash而已：
# Returns a hash of the attributes with unsaved changes indicating their original # values like &amp;lt;tt&amp;gt;attr =&amp;gt; original value&amp;lt;/tt&amp;gt;.</description></item><item><title>解读Rails - 处理异常</title><link>https://blog.hackerpie.com/posts/archive/jie-du-rails-chu-li-yi-chang/</link><pubDate>Wed, 05 Mar 2014 13:53:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/jie-du-rails-chu-li-yi-chang/</guid><description>此文翻译自Reading Rails - Handling Exceptions，限于本人水平，翻译不当之处，敬请指教！
我们今天开始会读一些Rails的源码。我们有双重的目的，先通过学习（Rails）如何处理异常，再扩展到整个Ruby中基础知识的学习。
Rails通过让你使用rescue_from方法，让你在你的controller里边为常见的异常定义处理方法。举例来说吧，你可以在用户试图访问他们尚未付费的功能时将他们重定向到指定的付费页面。
class ApplicationController # Redirect users if they try to use disabled features. rescue_from FeatureDisabledError, InsufficientAccessError do |ex| flash[:alert] = &amp;#34;Your account does not support #{ex.feature_name}&amp;#34; redirect_to &amp;#34;/pricing&amp;#34; end #... 我们将会探索Rails是如何定义异常处理器，如何将它们与具体的异常进行匹配，以及如何使用它们去rescue失败的action。
如果需要跟着我的步骤走，请使用qwandry打开每一个相关的代码库，或者直接从github查看源码即可。
定义处理器(Handlers) ActiveSupport包含了一个用于定义异常如何被处理的模块Rescuable。第一个需要了解的方法就是rescue_from。这个方法通过方法名或者代码块为你想rescue的异常注册处理器（提示：查看代码，请在命令行中输入qw activesupport）：
def rescue_from(*klasses, &amp;amp;block) options = klasses.extract_options! unless options.has_key?(:with) if block_given? options[:with] = block else #... 首先，*klasses接收数量不定的异常类，所以你可以进行类似rescue_from(FeatureDisabledError, InsufficientAccessError)这样的调用。它们将会被存放在一个数组里。
接下来，请留意extract_options!的使用。这是一个常见的用于从一个数组生成一个options哈希表的技巧。假如klasses里边的最后一个元素是一个哈希表，那么这个元素会被弹出数组。现在Rails将会使用:with项所指定的方法，或者是使用传递给rescue_from的代码块。Rails中的这种技巧创造了一个灵活的接口。
接着继续往下看这个方法，我们看到每一个异常类都被转换成一个String对象，我们待会便会看到为什么要这么做。
def rescue_from(*klasses, &amp;amp;block) #... key = if klass.is_a?(Class) &amp;amp;&amp;amp; klass &amp;lt;= Exception klass.</description></item><item><title>解读Rails - 适配器模式</title><link>https://blog.hackerpie.com/posts/archive/jie-du-rails-gua-pei-qi-mo-shi/</link><pubDate>Mon, 03 Mar 2014 13:40:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/jie-du-rails-gua-pei-qi-mo-shi/</guid><description>本文翻译自Reading Rails - The Adapter Pattern，限于本人水平有限，翻译不当之处，敬请指教！
今天我们暂时先放下具体的代码片段，我们将要对Rails中所实现的一个比较常见的设计模式进行一番探索，这个模式就是适配器模式（Adapter Pattern）。从一定的意义上来说，这次的探索并不全面，但是我希望能够突出一些实际的例子。
为了跟随本文的步骤，请使用qwandry打开相关的代码库，或者直接在Github上查看这些代码。
适配器模式 适配器模式可以用于对不同的接口进行包装以及提供统一的接口，或者是让某一个对象看起来像是另一个类型的对象。在静态类型的编程语言里，我们经常使用它去满足类型系统的特点，但是在类似Ruby这样的弱类型编程语言里，我们并不需要这么做。尽管如此，它对于我们来说还是有很多意义的。
当使用第三方类或者库的时候，我们经常从这个例子开始（start out fine）：
def find_nearest_restaurant(locator) locator.nearest(:restaurant, self.lat, self.lon) end 我们假设有一个针对locator的接口，但是如果我们想要find_nearest_restaurant能够支持另一个库呢？这个时候我们可能就会去尝试添加新的特殊的场景的处理：
def find_nearest_restaurant(locator) if locator.is_a? GeoFish locator.nearest(:restaurant, self.lat, self.lon) elsif locator.is_a? ActsAsFound locator.find_food(:lat =&amp;gt; self.lat, :lon =&amp;gt; self.lon) else raise NotImplementedError, &amp;#34;#{locator.class.name}is not supported.&amp;#34; end end 这是一个比较务实的解决方案。或许我们也不再需要考虑去支持另一个库了。也或许find_nearest_restaurant就是我们使用locator的唯一场景。
那假如你真的需要去支持一个新的locator，那又会是怎么样的呢？那就是你有三个特定的场景。再假如你需要实现find_nearest_hospital方法呢？这样你就需要在维护这三种特定的场景时去兼顾两个不同的地方。当你觉得这种解决方案不再可行的时候，你就需要考虑适配器模式了。
在这个例子中，我们可以为GeoFish以及ActsAsFound编写适配器，这样的话，在我们的其他代码中，我们就不需要了解我们当前正在使用的是哪个库了：
def find_nearest_hospital(locator) locator.find :type =&amp;gt; :hospital, :lat =&amp;gt; self.lat, :lon =&amp;gt; self.lon end locator = GeoFishAdapter.new(geo_fish_locator) find_nearest_hospital(locator) 特意假设的例子就到此为止，接下来让我们看看真实的代码。
MultiJSON ActiveSupport在做JSON格式的解码时，用到的是MultiJSON，这是一个针对JSON库的适配器。每一个库都能够解析JSON，但是做法却不尽相同。让我们分别看看针对oj和yajl的适配器。 (提示: 可在命令行中输入qw multi_json查看源码。)</description></item><item><title>解读Rails(系列翻译)</title><link>https://blog.hackerpie.com/posts/archive/jie-du-rails-xi-lie-fan-yi/</link><pubDate>Sun, 02 Mar 2014 13:40:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/jie-du-rails-xi-lie-fan-yi/</guid><description>解读Rails系列文章原文来自Reading Rails，我将尽我所能完成此系列所有文章的翻译。以下部分为原文系列对应翻译：
在我们的工作中，我们的大部分时间都是在使用各种各样的工具，但是你知不知道它们是如何工作的呢？
在这个系列里，我们通过阅读Ruby on Rails的源码去探索其内部的工作机制。我们将不仅仅只是学到与Rails有关的知识，我们也将见识到Ruby里边一些有趣的功能，以及一些有用的新技巧。
解读Rails - 适配器模式 解读Rails - 错误和验证器 解读Rails - 验证机制是如何配置的 解读Rails - Concern 解读Rails - 更多的Migrations 解读Rails - Migrations 解读Rails - 属性方法(Attribute Methods) 解读Rails - 跟踪model中属性（值）的变更 解读Rails - 处理异常</description></item><item><title>在Ruby中使用WebSocket</title><link>https://blog.hackerpie.com/posts/archive/zai-rubyzhong-shi-yong-websocket/</link><pubDate>Sat, 01 Mar 2014 21:31:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/zai-rubyzhong-shi-yong-websocket/</guid><description>声明: 此文翻译自WebSockets in Ruby， 限于本人才疏学浅，其中有翻译不当之处，敬请指出，感激不尽！
在我的主要工作中，需要构建一个一直占用相当大CPU时间片的数据系统。这个任务主要用于在地理编码以及local reference system(本地地理系统？)之间进行编码以及解码。举个例子，这个工作将帮助我们在系统中标记一条对应于街道上某个地点的记录，并且可以知道本地地理位置所对应的坐标。
在第一次的尝试中，我开发了一个用于地理编码的Ruby库以及一个简单的基于Sinatra的web服务。当时我的解决方案表现得还不错，直到后来客户要求对每一个鼠标滑过的事件进行交互。这个需求上的更改让我不得不再一次通过Javascript语言去构建一个同样用于地理编码的基础构件，在之后的一段时间里，一切也都表现得非常好。
而意料之中的是，我们再一次决定在系统中允许每个用户与多个街道关联。现在，每次下载800KB的数据（存储在索引数据库中，用于记录最新的会话信息）尚且可以承受；但是潜在上来说，几个MB的数据将是致命的，甚至软件也有可能在会话的响应之前被使用-而这只是用户所期待的功能之一。
我知道我们必须寻找一个完美的解决方案，并且使一切都是可以管理控制的。在以前，我涉足过WebSocket领域（比如node.js以及Socket.IO）并且知道相关的底层知识。从之前的搜索中，我意识到Ruby在这方面的欠缺，我很快又考虑通过在节点上的Javascript端口来实现需求。这样的想法使我非常激动。
可选方案 第一步是找出可用的方案。以下列举我找到的：
sinatra-websocket faye-websocket websocket-rails tubesock webmachine-ruby 在上述五种方案中，前三种方案都是基于事件机制的，而tubesock使用了rake hijacking技术，webmachine-ruby通过基于Celluloid::IO的HTTP服务器Reel提供WebSockets。
首先，考虑到我已经使用了Sinatra，于是我试用了sinatra-websocket。但是因为部分原因，我无法将连接方式迁移到WebSocket，所以我决定快速跳过。而且坦白说的话，我还直接跳过了faye-websocket。
接下来的两个备选方案遇到了同样的问题：在一个配置较低的Heroku的站点上启动Rails并且加载了整个系统之后，剩下的内存只够几十个客户端同时使用的了。除此之外，Rails的启动时间加上其他用于构建的时间偶尔会让Heroku认为系统中出现异常，结果导致进程在服务正常启动之前就已经被强行退出了。
假如你有所留意，那么你也就知道了，剩下的唯一一个方案，就是webmachine-ruby。
webmachine-ruby 配置webmachine-ruby的环境还是相对容易的。为了逐步进行，我首先把原来基于HTTP的服务迁移到它的资源结构。比起Rails以及Sinatra，它更加具有面向对象的味道。它的分发器是易于理解的，我非常喜欢通过visual debugger来摆玩这一切。
迁移到WebSocket上后，一切都变了。我能建议的（包括文档中说明的）就是，你完全可以跳过常规的基础配置，转而提供一个可调用的配置项，比如：
App = Webmachine::Application do |app| app.configure do |config| config.adapter = :Reel config.adapter_options[:websocket_handler] = proc do |websocket| websocket &amp;lt;&amp;lt; &amp;#34;hello, world&amp;#34; end end end 这是相当多的文档所提到的方法。因为它只期望handler支持#call方法，所以你可以写一个你自己的ad-hoc分发器：
class WebsocketHandler def call(websocket) message = websocket.read # do something with the message, call methods on other objects, log stuff, have your fun end end 很多文档并不提及一些套接字编程的基础。假如你发现你的handler被挂起并且不再处理响应，这意味着你需要重新修改程序，但是不需要为此感到烦恼：你只需要实现一个不断从套接字中读取信息并且让Celluloid::IO实现它的非阻塞魔术方法的循环就行了：</description></item><item><title>DIY an interesting timer through terminal-notifier and crontab under Mac OS X</title><link>https://blog.hackerpie.com/posts/archive/diy-an-interesting-timer-through-terminal-notifier-and-crontab-under-mac-os-x/</link><pubDate>Sun, 12 Jan 2014 15:51:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/diy-an-interesting-timer-through-terminal-notifier-and-crontab-under-mac-os-x/</guid><description>Today I will show you how to DIY an intersting notifier automatically running per hour under the Mac OS X 10.8 and higher, all we need are a terminal-notifier and the system built-in command line program named crontab.
1. Install the terminal-notifier terminal-notifier is a program written in the awesome Ruby program language, we can visit the releases page and download the newest version of terminal-notifier. Steps to install it: Please click the green button such as &amp;ldquo;terminal-notifier-1.</description></item><item><title>Split logs automatically every day</title><link>https://blog.hackerpie.com/posts/archive/split-logs-automatically-every-day/</link><pubDate>Sat, 07 Sep 2013 01:11:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/split-logs-automatically-every-day/</guid><description>Related resource(s):
&amp;ldquo;linuxcommand: logrotate&amp;rdquo;:http://linuxcommand.org/man_pages/logrotate8.htmllogrotate is designed to ease administration of systems that generate large numbers of log files. Normally, logrotate is run as a daily cron job.
Some important knowledges:
Any number of config files may be given. Later config files may override the options given in earlier files, so the order in which the logrotate config files are listed in is important. Normally, a single config file which includes any other config files which are needed should be used.</description></item><item><title>Backup database and other attachments in ROR</title><link>https://blog.hackerpie.com/posts/archive/backup-database-and-other-attachments-in-ror/</link><pubDate>Sat, 07 Sep 2013 00:26:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/backup-database-and-other-attachments-in-ror/</guid><description>Related Resources rsync:http://rsync.samba.org/ Crontab:http://unixhelp.ed.ac.uk/CGI/man-cgi?crontab+5 &amp;ldquo;Linux Crontab 定时任务 命令详解&amp;rdquo;:http://blog.csdn.net/tianlesoftware/article/details/5315039 [rubygem]&amp;ldquo;backup&amp;rdquo;:https://github.com/meskyanichi/backup [rubygem]&amp;ldquo;whenever&amp;rdquo;:https://github.com/javan/whenever Automatically backup on the remote server: Let&amp;rsquo;s firstly assumpt:
You have a site named &amp;ldquo;example.com&amp;quot;; You can login to it through a username &amp;ldquo;deploy&amp;rdquo;, and its password is &amp;ldquo;password&amp;quot;; You located the contents of your site in /var/www/example/; Your database server is Mysql, and the database for your site is example_production. 1. SSH login:
2. Install the backup:</description></item><item><title>Write css codes distinct from different pages</title><link>https://blog.hackerpie.com/posts/archive/write-css-codes-distinct-from-different-pages/</link><pubDate>Thu, 29 Aug 2013 15:32:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/write-css-codes-distinct-from-different-pages/</guid><description>In rails, there is normally a view corresponding to an action. So if you want to do some special styles based on different pages, you can render controller name and action name in your layout file like this:
&amp;lt;body class=&amp;#34;#{controller_name} #{action_name}&amp;#34;&amp;gt; &amp;lt;!-- Render something --&amp;gt; &amp;lt;/body&amp;gt; Let&amp;rsquo;s assumpt that your current page controller is ProductsController, and your action is index, then you can write your style codes like:
.products.index { /* some styles */ }</description></item><item><title>Failed to stop or restart Nginx server through serevice command</title><link>https://blog.hackerpie.com/posts/archive/failed-to-stop-or-restart-nginx-server-through-serevice-command/</link><pubDate>Fri, 09 Aug 2013 11:39:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/failed-to-stop-or-restart-nginx-server-through-serevice-command/</guid><description>Many people are accustomed to start a Nginx web server through init scripts and then they can control the state of the server through service command, such as sudo service nginx restart. But sometimes unobvious config error makes the scripts failed to work. Here I will show an error related to pid directive in the config file of nginx, which defaultly located at /opt/nginx/conf/nginx.conf. As ignored by many people, some init scripts assump there is a pid file of nginx located at /var/run/nginx.</description></item><item><title>export/import datas to/from a csv file</title><link>https://blog.hackerpie.com/posts/archive/export-slash-import-to-slash-from-a-csv-file/</link><pubDate>Thu, 25 Jul 2013 22:19:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/export-slash-import-to-slash-from-a-csv-file/</guid><description>今天需要给客户的网站做支持产品数据导出并且更新的功能，所以就涉及到了数据的导入导出了。在经过一番对比之下，果断使用csv格式文件作为数据导入导出的载体。导出csv文件 与csv文件主要相关的类是CSV,此类在ruby的标准库中被定义，所以只要在代码开头引入相关文件即可:
require &amp;#39;csv&amp;#39; 接着需要创建csv文件，并在其中写入数据:
CSV.open &amp;#34;path/to/csv/file&amp;#34;, &amp;#34;wb&amp;#34;, :col_sep =&amp;gt; &amp;#39;|&amp;#39; do |csv| csv &amp;lt;&amp;lt; [&amp;#34;one&amp;#34;, &amp;#34;row&amp;#34;, &amp;#34;of&amp;#34;, &amp;#34;contents&amp;#34;] end ** 这里有几个细节值得一提：** open方法是用于对CSV文件进行写操作的主要接口,对CSV文件进行写操作都应该使用此方法; &amp;lt;&amp;lt; 操作符支持将字符串数组写入到csv文件,一个数组为一行，数组中的一个字符串为一个单元(field); open方法的第三个参数是一个哈希，用于为打开的文件指定初始化赋值，具体可用的option以及其值可参考new方法的说明:http://www.ruby-doc.org/stdlib-1.9.3/libdoc/csv/rdoc/CSV.html#method-c-new :col_sep用于指定文件中每一行中的每个单元之间的分隔符，当通过字符串数组想文件中添加新行的时候，CSV将会在数组元素也就是每一行的单元之间插入指定的分隔符，分隔符需要尽量避开已经在待导出数据中存在的字符，以免后续导入的时候发生歧义。 导入csv文件 导入csv除了需要用到相关的类CSV，还可能用到的类是CSV::Row，前者提供打开文件以及将文件按行分隔的方法foreach，foreach将分隔后的行逐行分配到CSV::Row的实例中，通过调用CSV::Row的实例方法field可对每个单元进行读取。假设我有这样一个csv文件：
id | value 1234 | hello 2345 | world 则相关的代码如下：
CSV.foreach(&amp;#39;path/to/file&amp;#39;), :col_sep =&amp;gt; &amp;#39;|&amp;#39;, :headers =&amp;gt; :first_row do |row| # use datas of each row id = row.field &amp;#39;id&amp;#39; value = row.field &amp;#39;value&amp;#39; end ** 同样这里也有几个细节需要注意:** foreach是CSV类中用于读取文件的标准方法； 通过指定:col_sep =&amp;gt; '|'可使CSV按照指定的分隔符分隔好文件内容，方便后面field方法的调用； 指定:headers =&amp;gt; :first_row可使CSV将第一行视为文件的headers，并且结合:col_sep的设定可以将headers进行分割，这两个是field方法的基础。 Related links: http://www.</description></item><item><title>Rails HTTP Status Code to Symbol Mapping</title><link>https://blog.hackerpie.com/posts/archive/rails-http-status-code-to-symbol-mapping/</link><pubDate>Wed, 24 Jul 2013 15:00:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/rails-http-status-code-to-symbol-mapping/</guid><description>Sometimes you might need to set the HTTP response head with different status to specify the different handled results. You can do this in Rails by add a :status to a rails method such as #respond_with. Here list all the maps:
{% gist 2405434 %}
Addtionally, you can view all these on your local machine by installing the gem cheat, and see all status codes using command cheat status_codes.
All these informations come from internet, the codes file comes from ktkaushik&amp;rsquo;s gist and other information comes from Cody Fauser&amp;rsquo;s post</description></item><item><title>Delete multiple git remote branches by prefixing all refs with a colon</title><link>https://blog.hackerpie.com/posts/archive/delete-multiple-git-remote-branches-by-prefixing-all-refs-with-a-colon/</link><pubDate>Sun, 21 Jul 2013 15:46:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/delete-multiple-git-remote-branches-by-prefixing-all-refs-with-a-colon/</guid><description>This article shows how to delete multiple remote branches in Git.
git push origin :branch-1 :branch-2 [:other-branches] Remember the colon :</description></item><item><title>Track Original Repo When Fork</title><link>https://blog.hackerpie.com/posts/archive/track-original-repo-when-fork/</link><pubDate>Sun, 21 Jul 2013 10:24:00 +0800</pubDate><guid>https://blog.hackerpie.com/posts/archive/track-original-repo-when-fork/</guid><description>{% gist 5908916 %}</description></item></channel></rss>